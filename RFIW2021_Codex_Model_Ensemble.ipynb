{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RFIW2021_Codex Model Ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwA6nLUGdCbt",
        "outputId": "bfb4c661-0534-4c55-91f0-505e0f9fe8c5"
      },
      "source": [
        "from collections import defaultdict\n",
        "from glob import glob\n",
        "from random import choice, sample\n",
        "\n",
        "import tensorflow as tf\n",
        "#import keras\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate,Activation, Multiply, Dropout, Subtract,MaxPooling2D,ZeroPadding2D,Convolution2D\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_vggface.utils import preprocess_input\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from deepface import DeepFace\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input as preprocess_input_facenet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-09-15 01:36:47.273415: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqeDnzSFKODr"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import seaborn as sns\n",
        "import pandas as pd\n",
        "#import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from random import choice, sample\n",
        "import random \n",
        "import cv2\n",
        "from imageio import imread\n",
        "# from keras.preprocessing.text import Tokenizer, one_hot\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.models import Model, load_model\n",
        "# from keras import regularizers\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dropout, BatchNormalization,Dense, concatenate, Flatten, Conv1D\n",
        "# from keras.optimizers import RMSprop, Adam\n",
        "# from keras_vggface.vggface import VGGFace\n",
        "from glob import glob\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D, Lambda, Reshape\n",
        "from collections import defaultdict\n",
        "# from keras_vggface.utils import preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiMxS4WQJQfq",
        "outputId": "5455644c-e7c0-49b3-a892-8269ccf8d554"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "print(tf.test.is_gpu_available())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 3671733584146702053\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14646682624\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 11999655187236681609\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n",
            "True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-09-15 01:36:53.855668: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-09-15 01:36:53.856737: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-09-15 01:36:53.888092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:36:53.888712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-09-15 01:36:53.888740: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-09-15 01:36:53.891682: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-09-15 01:36:53.891771: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-09-15 01:36:53.892982: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-09-15 01:36:53.893295: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-09-15 01:36:53.896405: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-09-15 01:36:53.897181: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-09-15 01:36:53.897333: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-09-15 01:36:53.897427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:36:53.898074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:36:53.898654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-09-15 01:36:53.898687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-09-15 01:36:54.618364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-09-15 01:36:54.618396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-09-15 01:36:54.618404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-09-15 01:36:54.618716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:36:54.619414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:36:54.620052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:36:54.620663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 13968 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2021-09-15 01:36:54.622502: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-09-15 01:36:54.622640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:36:54.623221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-09-15 01:36:54.623254: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-09-15 01:36:54.623295: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-09-15 01:36:54.623306: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-09-15 01:36:54.623317: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-09-15 01:36:54.623327: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-09-15 01:36:54.623337: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-09-15 01:36:54.623348: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-09-15 01:36:54.623359: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-09-15 01:36:54.623424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:36:54.624027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:36:54.624557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-09-15 01:36:54.624577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-09-15 01:36:54.624583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-09-15 01:36:54.624587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-09-15 01:36:54.624663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:36:54.625309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:36:54.625862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 13968 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOsYNe8ylWen"
      },
      "source": [
        "train_folders_path = \"./kinship_dataset/train-faces/\"\n",
        "val_folders_path   = \"./kinship_dataset/val-faces/\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wMzjs_Cdl68",
        "outputId": "4b3fa903-edbf-41ae-82b7-5b6356db8b63"
      },
      "source": [
        "all_images_train = list(set(glob(train_folders_path + \"*/*/*.jpg\")))\n",
        "all_images_val   = list(set(glob(val_folders_path + \"*/*/*.jpg\")))\n",
        "print(len(all_images_train))\n",
        "print(len(all_images_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21920\n",
            "5045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIk3MvDyb1ZE",
        "outputId": "39f614f8-885c-468f-ce97-a16919b3b973"
      },
      "source": [
        "all_images_train[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['./kinship_dataset/train-faces/F0844/unrelated_and_nonfaces/P08912_face0.jpg',\n",
              " './kinship_dataset/train-faces/F0733/MID1/P07684_face0.jpg']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkGUh3GawCqp"
      },
      "source": [
        "import random\n",
        "\n",
        "def process_data(image_path_list):\n",
        "    #people = set([])\n",
        "    #person_to_images_map = defaultdict(list)\n",
        "    family_faces = defaultdict(list)\n",
        "    #family_size = {}\n",
        "\n",
        "    for x in image_path_list: \n",
        "        splits = x.split(\"/\")\n",
        "\n",
        "        family = splits[-3]\n",
        "        # family_size.setdefault(family, 0)\n",
        "        # family_size[family] += 1\n",
        "\n",
        "        person = splits[-3] + \"/\" + splits[-2]\n",
        "        #people.add(person)\n",
        "\n",
        "        #person_to_images_map[person].append(x)\n",
        "        family_faces[family].append(x)\n",
        "\n",
        "    return family_faces"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhGvMzHbVRrY",
        "outputId": "feb5e6ed-353e-44b4-c4ee-243b629aa27f"
      },
      "source": [
        "# process train data \n",
        "family_faces_train = process_data(all_images_train)\n",
        "families_train = list(family_faces_train.keys())\n",
        "random.shuffle(families_train)\n",
        "print(len(families_train))\n",
        "\n",
        "# process val data \n",
        "family_faces_val = process_data(all_images_val)\n",
        "families_val = list(family_faces_val.keys())\n",
        "random.shuffle(families_val)\n",
        "print(len(families_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "571\n",
            "192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SihpfJ3G8cqJ"
      },
      "source": [
        "family_faces = family_faces_train\n",
        "family_faces.update(family_faces_val)\n",
        "family_faces_val = None\n",
        "family_faces_train = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vW2HpCbWKGj"
      },
      "source": [
        "# provided label is not used \n",
        "\n",
        "# train_file_path = \"./train-pairs.csv\"\n",
        "# relationships_raw = pd.read_csv(train_file_path)\n",
        "# relationships_raw = list(zip(relationships_raw.fid1.values, relationships_raw.fid2.values))\n",
        "# print(len(relationships_raw))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU8LDCW9QH0T"
      },
      "source": [
        "Create dataset (mother-father are kin in this)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-bXSNzhsfiV"
      },
      "source": [
        "#Visualize one family\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.image as mpimg\n",
        "\n",
        "# for face in family_faces_train['F0207']:\n",
        "#   plt.figure()\n",
        "#   img = mpimg.imread(face)\n",
        "#   plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gur9VXFs3fnP"
      },
      "source": [
        "def read_img(path):\n",
        "    img = image.load_img(path, target_size=(224, 224))\n",
        "    img = np.array(img).astype(np.float)\n",
        "    return preprocess_input(img, version=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqfvOnK-3jdz"
      },
      "source": [
        "# all persons within the same family are considered kin\n",
        "from random import choices\n",
        "\n",
        "def gen(list_tuples, batch_size, read_img):\n",
        "    while True:\n",
        "        # get families\n",
        "        batch_tuples = choices(list_tuples, k=batch_size//2)\n",
        "        kin_tuples = [(batch_tuples[i], batch_tuples[i]) for i in range(len(batch_tuples))]\n",
        "\n",
        "        # combine families\n",
        "        batch_tuples = sample(list_tuples, k=batch_size)\n",
        "        batch_tuples = [(batch_tuples[i], batch_tuples[i+1]) for i in range(0, len(batch_tuples), 2)]\n",
        "\n",
        "        # make labels\n",
        "        labels = [1]*(batch_size//2) + [0]*(batch_size//2)\n",
        "        batch_tuples = kin_tuples + batch_tuples\n",
        "\n",
        "\n",
        "        # get people in families\n",
        "        tuples = []\n",
        "        for a in batch_tuples:\n",
        "          notfound = True\n",
        "          while(notfound):\n",
        "            p1 = choice(family_faces[a[0]])\n",
        "            p2 = choice(family_faces[a[1]])\n",
        "            if \"/\".join(p1.split(\"/\")[-3:-1]) != \"/\".join(p2.split(\"/\")[-3:-1]):\n",
        "              tuples.append((p1, p2))\n",
        "              notfound = False\n",
        "        batch_tuples = [(a[0], a[1], b) for (a, b) in zip(tuples, labels)]\n",
        "        random.shuffle(batch_tuples)\n",
        "        X1 = [x[0] for x in batch_tuples]\n",
        "        input_3 = np.array([read_img(x) for x in X1])\n",
        "        \n",
        "\n",
        "        X2 = [x[1] for x in batch_tuples]\n",
        "        input_4 = np.array([read_img(x) for x in X2])\n",
        "        labels = [x[2] for x in batch_tuples]\n",
        "\n",
        "        yield [input_3, input_4], np.array(labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npstosjSZRa3"
      },
      "source": [
        "def model_1():\n",
        "    input_1 = Input(shape=(224, 224, 3))\n",
        "    input_2 = Input(shape=(224, 224, 3))\n",
        "\n",
        "    vgg_model = VGGFace(model='resnet50', input_shape=(224, 224, 3), include_top=False, pooling='None')\n",
        "    out = vgg_model.get_layer('avg_pool').output\n",
        "    base_model = Model(vgg_model.input, out)\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x1 = base_model(input_1)\n",
        "    x2 = base_model(input_2)\n",
        "    x1 = Flatten()(x1)\n",
        "    x2 = Flatten()(x2)\n",
        "\n",
        "    x3 = Subtract()([x1, x2])\n",
        "    x3 = Multiply()([x3, x3])\n",
        "\n",
        "    x1_ = Multiply()([x1, x1])\n",
        "    x2_ = Multiply()([x2, x2])\n",
        "    \n",
        "    x4  = Subtract()([x1_, x2_])  \n",
        "    x4 = Lambda(lambda tensor  : K.abs(tensor))(x4)\n",
        "\n",
        "    x5 = Multiply()([x1, x2])\n",
        "\n",
        "    x   = Concatenate(axis=-1)([x5, x4, x3])\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.5)(x)\n",
        "    x   = Dense(256, activation=\"relu\")(x)\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.3)(x)\n",
        "    x   = Dense(128, activation=\"relu\")(x)\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.3)(x) \n",
        "    out = Dense(1, activation=\"sigmoid\")(x)\n",
        "  \n",
        "\n",
        "    model = Model([input_1, input_2], out)\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.001))\n",
        "    #model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gYStB2sncLA"
      },
      "source": [
        "def model_2():\n",
        "    input_1 = Input(shape=(224, 224, 3))\n",
        "    input_2 = Input(shape=(224, 224, 3))\n",
        "\n",
        "    vgg_model = VGGFace(model='senet50', input_shape=(224, 224, 3), include_top=False, pooling='None')\n",
        "    out = vgg_model.get_layer('avg_pool').output\n",
        "    base_model = Model(vgg_model.input, out)\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x1 = base_model(input_1)\n",
        "    x2 = base_model(input_2)\n",
        "    x1 = Flatten()(x1)\n",
        "    x2 = Flatten()(x2)\n",
        "\n",
        "    x3 = Subtract()([x1, x2])\n",
        "    x3 = Multiply()([x3, x3])\n",
        "\n",
        "    \n",
        "    x1_ = Multiply()([x1, x1])\n",
        "    x2_ = Multiply()([x2, x2])\n",
        "    \n",
        "    x4  = Subtract()([x1_, x2_])  \n",
        "    x4 = Lambda(lambda tensor  : K.abs(tensor))(x4)\n",
        "\n",
        "    x5 = Multiply()([x1, x2])\n",
        "\n",
        "    x   = Concatenate(axis=-1)([x5, x4, x3])\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.5)(x)\n",
        "    x   = Dense(256, activation=\"relu\")(x)\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.3)(x)\n",
        "    x   = Dense(128, activation=\"relu\")(x)\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.3)(x) \n",
        "    out = Dense(1, activation=\"sigmoid\")(x)\n",
        "  \n",
        "\n",
        "    model = Model([input_1, input_2], out)\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.001))\n",
        "    #model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSy9gc4KBg7C"
      },
      "source": [
        "def model_3():\n",
        "    input_1 = Input(shape=(224, 224, 3))\n",
        "    input_2 = Input(shape=(224, 224, 3))\n",
        "\n",
        "    vgg_model = VGGFace(model='resnet50', input_shape=(224, 224, 3), include_top=False, pooling='None')\n",
        "    out = vgg_model.get_layer('avg_pool').output\n",
        "    base_model = Model(vgg_model.input, out)\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x1 = base_model(input_1)\n",
        "    x2 = base_model(input_2)\n",
        "    x1 = Flatten()(x1)\n",
        "    x2 = Flatten()(x2)\n",
        "\n",
        "    x3 = Subtract()([x1, x2])\n",
        "    x3 = Multiply()([x3, x3])\n",
        "\n",
        "    x1_ = Multiply()([x1, x1])\n",
        "    x2_ = Multiply()([x2, x2])\n",
        "    \n",
        "    x4  = Subtract()([x1_, x2_])  \n",
        "    x4 = Lambda(lambda tensor  : K.abs(tensor))(x4)\n",
        "\n",
        "    x5 = Multiply()([x1, x2])\n",
        "\n",
        "    x   = Concatenate(axis=-1)([x5, x4, x3])\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.5)(x)\n",
        "    x   = Dense(256, activation=\"relu\")(x)\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.3)(x)\n",
        "    x   = Dense(256, activation=\"relu\")(x)\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.3)(x)\n",
        "    x   = Dense(128, activation=\"relu\")(x)\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.3)(x)\n",
        "    x   = Dense(128, activation=\"relu\")(x)\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.3)(x) \n",
        "    out = Dense(1, activation=\"sigmoid\")(x)\n",
        "  \n",
        "\n",
        "    model = Model([input_1, input_2], out)\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.001))\n",
        "    #model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjPkpImEBvUX"
      },
      "source": [
        "def model_4():\n",
        "    input_1 = Input(shape=(224, 224, 3))\n",
        "    input_2 = Input(shape=(224, 224, 3))\n",
        "\n",
        "    vgg_model = VGGFace(model='senet50', input_shape=(224, 224, 3), include_top=False, pooling='None')\n",
        "    out = vgg_model.get_layer('avg_pool').output\n",
        "    base_model = Model(vgg_model.input, out)\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x1 = base_model(input_1)\n",
        "    x2 = base_model(input_2)\n",
        "    x1 = Flatten()(x1)\n",
        "    x2 = Flatten()(x2)\n",
        "\n",
        "    x3 = Subtract()([x1, x2])\n",
        "    x3 = Multiply()([x3, x3])\n",
        "\n",
        "    x1_ = Multiply()([x1, x1])\n",
        "    x2_ = Multiply()([x2, x2])\n",
        "    \n",
        "    x4  = Subtract()([x1_, x2_])  \n",
        "    x4 = Lambda(lambda tensor  : K.abs(tensor))(x4)\n",
        "\n",
        "    x5 = Multiply()([x1, x2])\n",
        "\n",
        "    x   = Concatenate(axis=-1)([x5, x4, x3])\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.5)(x)\n",
        "    x   = Dense(256, activation=\"relu\")(x)\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.3)(x)\n",
        "    x   = Dense(256, activation=\"relu\")(x)\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.3)(x)\n",
        "    x   = Dense(128, activation=\"relu\")(x)\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.3)(x)\n",
        "    x   = Dense(128, activation=\"relu\")(x)\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Dropout(0.3)(x) \n",
        "    out = Dense(1, activation=\"sigmoid\")(x)\n",
        "  \n",
        "\n",
        "    model = Model([input_1, input_2], out)\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.001))\n",
        "    #model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBOrbUX81CxW"
      },
      "source": [
        "def train(model, model_name, model_max_acc_dict, \n",
        "          read_img_for_train, read_img_for_val, learning_rate, early_stop_threshold=10, \n",
        "          generator_func=gen, train_relationships=[], val_relationships=[]): \n",
        "    print(\"Training model \" + model_name)\n",
        "    weight_file_path = \"./models/\" + model_name + \".weights\"\n",
        "\n",
        "    epoch_max = 200\n",
        "    early_stop_threshold = early_stop_threshold\n",
        "    reduce_threshold = 3\n",
        "\n",
        "    early_stop_count = 1\n",
        "    reduce_count = 1\n",
        "    val_acc_max = -1\n",
        "    acc_max = -1\n",
        "\n",
        "    for epoch_index in range(1, epoch_max): \n",
        "        model.fit_generator(generator=generator_func(train_relationships, batch_size=32, read_img=read_img_for_train), \n",
        "                  validation_data=generator_func(val_relationships, batch_size=16, read_img=read_img_for_val),\n",
        "                  epochs=1, \n",
        "                  verbose=1, \n",
        "                  steps_per_epoch=200,\n",
        "                  validation_steps=50)\n",
        "        \n",
        "        new_val_acc = model.history.history[\"val_acc\"][-1]\n",
        "        if new_val_acc > val_acc_max: \n",
        "            val_acc_max = new_val_acc \n",
        "            model_max_acc_dict[model_name] = val_acc_max\n",
        "            np.save(weight_file_path, np.array(model.get_weights()))\n",
        "            print(\"saved weights for model at \" + weight_file_path)\n",
        "            early_stop_count = 1\n",
        "        else: \n",
        "            early_stop_count += 1\n",
        "        if early_stop_count >= early_stop_threshold: \n",
        "            print(\"end training for model early\")\n",
        "            break \n",
        "        \n",
        "        new_acc = model.history.history[\"acc\"][-1]\n",
        "        if new_acc > acc_max: \n",
        "            acc_max = new_acc\n",
        "            reduce_count = 1\n",
        "        else: \n",
        "            reduce_count += 1\n",
        "        if reduce_count >= reduce_threshold: \n",
        "            learning_rate = learning_rate * 0.1\n",
        "            model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(learning_rate))\n",
        "            print(\"reduced learning rate to \" + str(learning_rate))\n",
        "\n",
        "    json_object = json.dumps(model_max_acc_dict)\n",
        "    with open(\"./models/model_max_acc_dict.json\", \"w\") as outfile:\n",
        "        outfile.write(json_object)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAdvPsM-n3W_",
        "outputId": "864bc756-28b2-4ffb-c162-c0f2a087728e"
      },
      "source": [
        "import json\n",
        "from pprint import pprint as pp\n",
        "model_max_acc_dict = {}\n",
        "with open('./models/model_max_acc_dict.json', 'r') as openfile:\n",
        "    model_max_acc_dict = json.load(openfile)\n",
        "    pp(model_max_acc_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'model_1.1': 0.6812499761581421,\n",
            " 'model_1.2': 0.699999988079071,\n",
            " 'model_1.3': 0.7174999713897705,\n",
            " 'model_1.4': 0.6962500214576721,\n",
            " 'model_2.1': 0.6875,\n",
            " 'model_2.2': 0.7212499976158142,\n",
            " 'model_2.3': 0.7275000214576721,\n",
            " 'model_2.4': 0.6899999976158142,\n",
            " 'model_3.1': 0.7099999785423279,\n",
            " 'model_3.2': 0.699999988079071,\n",
            " 'model_3.3': 0.6762499809265137,\n",
            " 'model_3.4': 0.7024999856948853,\n",
            " 'model_4.1': 0.7137500047683716,\n",
            " 'model_4.2': 0.7087500095367432,\n",
            " 'model_4.3': 0.6912500262260437,\n",
            " 'model_4.4': 0.668749988079071}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0ycoZ0zF0xv"
      },
      "source": [
        "families =  families_train + families_val\n",
        "families_train = families_val = None\n",
        "random.shuffle(families)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1dZKtEZJWOf",
        "outputId": "1b463cf7-bf72-478e-867e-269dab4699f4"
      },
      "source": [
        "# train the same model multiples times based on different split of train and validation data\n",
        "import gc\n",
        "\n",
        "k_fold = 4\n",
        "\n",
        "size_ = len(families) // k_fold\n",
        "groups = [[x, x + size_] for x in range(0, len(families), size_)]\n",
        "groups[-1][1] = len(families)\n",
        "\n",
        "model_factory = model_1\n",
        "model_name = \"model_1.\"\n",
        "model = None\n",
        "\n",
        "for i in range(0, k_fold): \n",
        "    val_start_index, val_end_index = groups[i]\n",
        "    val_relationships = families[val_start_index:val_end_index]\n",
        "    train_relationships = families[0:val_start_index] \\\n",
        "                        + families[val_end_index:len(families)]\n",
        "    del model\n",
        "    gc.collect()\n",
        "    model = model_factory()\n",
        "    train(model, model_name + str(i+1), model_max_acc_dict, \n",
        "          read_img, read_img, 0.001, 10, gen, \n",
        "          train_relationships, val_relationships)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model model_1.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 63s 213ms/step - loss: 0.8798 - acc: 0.5531 - val_loss: 0.7205 - val_acc: 0.5863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.1.weights\n",
            "200/200 [==============================] - 40s 201ms/step - loss: 0.6923 - acc: 0.6112 - val_loss: 0.6472 - val_acc: 0.6288\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.1.weights\n",
            "200/200 [==============================] - 39s 195ms/step - loss: 0.6428 - acc: 0.6278 - val_loss: 0.6129 - val_acc: 0.6350\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.1.weights\n",
            "200/200 [==============================] - 39s 195ms/step - loss: 0.6134 - acc: 0.6516 - val_loss: 0.6179 - val_acc: 0.6325\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.6136 - acc: 0.6447 - val_loss: 0.6195 - val_acc: 0.6325\n",
            "200/200 [==============================] - 40s 199ms/step - loss: 0.6020 - acc: 0.6622 - val_loss: 0.5978 - val_acc: 0.6575\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.1.weights\n",
            "200/200 [==============================] - 39s 195ms/step - loss: 0.5999 - acc: 0.6595 - val_loss: 0.6078 - val_acc: 0.6463\n",
            "200/200 [==============================] - 39s 194ms/step - loss: 0.5963 - acc: 0.6595 - val_loss: 0.5927 - val_acc: 0.6475\n",
            "reduced learning rate to 0.0001\n",
            "200/200 [==============================] - 45s 201ms/step - loss: 0.5919 - acc: 0.6588 - val_loss: 0.5986 - val_acc: 0.6725\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.1.weights\n",
            "reduced learning rate to 1e-05\n",
            "200/200 [==============================] - 45s 201ms/step - loss: 0.5859 - acc: 0.6703 - val_loss: 0.6108 - val_acc: 0.6625\n",
            "200/200 [==============================] - 40s 202ms/step - loss: 0.5840 - acc: 0.6828 - val_loss: 0.6149 - val_acc: 0.6350\n",
            "200/200 [==============================] - 40s 199ms/step - loss: 0.5901 - acc: 0.6705 - val_loss: 0.5968 - val_acc: 0.6525\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5835 - acc: 0.6761 - val_loss: 0.5914 - val_acc: 0.6575\n",
            "reduced learning rate to 1.0000000000000002e-06\n",
            "200/200 [==============================] - 46s 208ms/step - loss: 0.6066 - acc: 0.6538 - val_loss: 0.6003 - val_acc: 0.6700\n",
            "reduced learning rate to 1.0000000000000002e-07\n",
            "200/200 [==============================] - 45s 202ms/step - loss: 0.5966 - acc: 0.6689 - val_loss: 0.5852 - val_acc: 0.6687\n",
            "reduced learning rate to 1.0000000000000004e-08\n",
            "200/200 [==============================] - 45s 202ms/step - loss: 0.5809 - acc: 0.6822 - val_loss: 0.6031 - val_acc: 0.6363\n",
            "reduced learning rate to 1.0000000000000005e-09\n",
            "200/200 [==============================] - 45s 203ms/step - loss: 0.5908 - acc: 0.6637 - val_loss: 0.6074 - val_acc: 0.6650\n",
            "reduced learning rate to 1.0000000000000006e-10\n",
            "200/200 [==============================] - 46s 205ms/step - loss: 0.5810 - acc: 0.6756 - val_loss: 0.5873 - val_acc: 0.6812\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.1.weights\n",
            "reduced learning rate to 1.0000000000000006e-11\n",
            "200/200 [==============================] - 46s 207ms/step - loss: 0.5913 - acc: 0.6730 - val_loss: 0.5641 - val_acc: 0.6787\n",
            "reduced learning rate to 1.0000000000000006e-12\n",
            "200/200 [==============================] - 45s 203ms/step - loss: 0.5840 - acc: 0.6768 - val_loss: 0.5999 - val_acc: 0.6637\n",
            "reduced learning rate to 1.0000000000000007e-13\n",
            "200/200 [==============================] - 45s 204ms/step - loss: 0.5791 - acc: 0.6709 - val_loss: 0.5999 - val_acc: 0.6450\n",
            "reduced learning rate to 1.0000000000000008e-14\n",
            "200/200 [==============================] - 46s 209ms/step - loss: 0.6004 - acc: 0.6556 - val_loss: 0.5674 - val_acc: 0.6737\n",
            "reduced learning rate to 1.0000000000000009e-15\n",
            "200/200 [==============================] - 45s 201ms/step - loss: 0.5928 - acc: 0.6632 - val_loss: 0.5998 - val_acc: 0.6750\n",
            "reduced learning rate to 1.000000000000001e-16\n",
            "200/200 [==============================] - 45s 204ms/step - loss: 0.5911 - acc: 0.6707 - val_loss: 0.5949 - val_acc: 0.6438\n",
            "reduced learning rate to 1.000000000000001e-17\n",
            "200/200 [==============================] - 45s 202ms/step - loss: 0.5771 - acc: 0.6733 - val_loss: 0.5702 - val_acc: 0.6800\n",
            "reduced learning rate to 1.000000000000001e-18\n",
            "200/200 [==============================] - 47s 212ms/step - loss: 0.5874 - acc: 0.6679 - val_loss: 0.6172 - val_acc: 0.6575\n",
            "reduced learning rate to 1.000000000000001e-19\n",
            "200/200 [==============================] - 47s 210ms/step - loss: 0.5882 - acc: 0.6711 - val_loss: 0.5868 - val_acc: 0.6812\n",
            "end training for model early\n",
            "Training model model_1.2\n",
            "200/200 [==============================] - 44s 196ms/step - loss: 0.8619 - acc: 0.5412 - val_loss: 0.6144 - val_acc: 0.6575\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.2.weights\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.6801 - acc: 0.6112 - val_loss: 0.6312 - val_acc: 0.6150\n",
            "200/200 [==============================] - 40s 198ms/step - loss: 0.6415 - acc: 0.6333 - val_loss: 0.6089 - val_acc: 0.6438\n",
            "200/200 [==============================] - 38s 190ms/step - loss: 0.6230 - acc: 0.6414 - val_loss: 0.5882 - val_acc: 0.6587\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.2.weights\n",
            "200/200 [==============================] - 39s 196ms/step - loss: 0.6056 - acc: 0.6656 - val_loss: 0.5812 - val_acc: 0.6888\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.2.weights\n",
            "200/200 [==============================] - 40s 199ms/step - loss: 0.6050 - acc: 0.6581 - val_loss: 0.5955 - val_acc: 0.6662\n",
            "200/200 [==============================] - 38s 190ms/step - loss: 0.5942 - acc: 0.6684 - val_loss: 0.6091 - val_acc: 0.6425\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5940 - acc: 0.6695 - val_loss: 0.5953 - val_acc: 0.6562\n",
            "200/200 [==============================] - 38s 190ms/step - loss: 0.5910 - acc: 0.6666 - val_loss: 0.6120 - val_acc: 0.6587\n",
            "200/200 [==============================] - 38s 192ms/step - loss: 0.5936 - acc: 0.6653 - val_loss: 0.5878 - val_acc: 0.6575\n",
            "reduced learning rate to 0.0001\n",
            "200/200 [==============================] - 43s 194ms/step - loss: 0.5912 - acc: 0.6743 - val_loss: 0.5974 - val_acc: 0.6850\n",
            "200/200 [==============================] - 38s 189ms/step - loss: 0.5820 - acc: 0.6742 - val_loss: 0.5780 - val_acc: 0.7000\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.2.weights\n",
            "200/200 [==============================] - 38s 189ms/step - loss: 0.5765 - acc: 0.6850 - val_loss: 0.5922 - val_acc: 0.6513\n",
            "200/200 [==============================] - 38s 189ms/step - loss: 0.5779 - acc: 0.6830 - val_loss: 0.5799 - val_acc: 0.6700\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5748 - acc: 0.6784 - val_loss: 0.5705 - val_acc: 0.6787\n",
            "reduced learning rate to 1e-05\n",
            "200/200 [==============================] - 44s 197ms/step - loss: 0.5625 - acc: 0.6934 - val_loss: 0.5765 - val_acc: 0.6687\n",
            "200/200 [==============================] - 38s 190ms/step - loss: 0.5691 - acc: 0.6847 - val_loss: 0.5959 - val_acc: 0.6712\n",
            "200/200 [==============================] - 38s 189ms/step - loss: 0.5704 - acc: 0.6894 - val_loss: 0.5817 - val_acc: 0.6750\n",
            "200/200 [==============================] - 38s 188ms/step - loss: 0.5726 - acc: 0.6911 - val_loss: 0.6014 - val_acc: 0.6675\n",
            "200/200 [==============================] - 38s 190ms/step - loss: 0.5709 - acc: 0.6892 - val_loss: 0.5729 - val_acc: 0.6600\n",
            "200/200 [==============================] - 38s 188ms/step - loss: 0.5801 - acc: 0.6770 - val_loss: 0.5905 - val_acc: 0.6750\n",
            "end training for model early\n",
            "Training model model_1.3\n",
            "200/200 [==============================] - 43s 193ms/step - loss: 0.8037 - acc: 0.5618 - val_loss: 0.6139 - val_acc: 0.6550\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.3.weights\n",
            "200/200 [==============================] - 38s 193ms/step - loss: 0.6820 - acc: 0.6053 - val_loss: 0.6123 - val_acc: 0.6325\n",
            "200/200 [==============================] - 38s 192ms/step - loss: 0.6426 - acc: 0.6291 - val_loss: 0.5810 - val_acc: 0.6700\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.3.weights\n",
            "200/200 [==============================] - 38s 192ms/step - loss: 0.6247 - acc: 0.6408 - val_loss: 0.5816 - val_acc: 0.6850\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.3.weights\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.6122 - acc: 0.6495 - val_loss: 0.5485 - val_acc: 0.7175\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.3.weights\n",
            "200/200 [==============================] - 39s 194ms/step - loss: 0.6008 - acc: 0.6553 - val_loss: 0.5606 - val_acc: 0.6862\n",
            "200/200 [==============================] - 38s 190ms/step - loss: 0.6047 - acc: 0.6536 - val_loss: 0.5350 - val_acc: 0.7163\n",
            "200/200 [==============================] - 38s 190ms/step - loss: 0.5961 - acc: 0.6589 - val_loss: 0.5971 - val_acc: 0.6400\n",
            "200/200 [==============================] - 38s 190ms/step - loss: 0.5943 - acc: 0.6691 - val_loss: 0.5842 - val_acc: 0.6750\n",
            "200/200 [==============================] - 38s 189ms/step - loss: 0.5886 - acc: 0.6719 - val_loss: 0.5766 - val_acc: 0.6700\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5964 - acc: 0.6727 - val_loss: 0.5683 - val_acc: 0.6888\n",
            "200/200 [==============================] - 38s 192ms/step - loss: 0.5889 - acc: 0.6702 - val_loss: 0.5835 - val_acc: 0.6812\n",
            "200/200 [==============================] - 37s 188ms/step - loss: 0.5865 - acc: 0.6783 - val_loss: 0.5627 - val_acc: 0.6975\n",
            "200/200 [==============================] - 38s 189ms/step - loss: 0.5820 - acc: 0.6786 - val_loss: 0.5626 - val_acc: 0.6950\n",
            "end training for model early\n",
            "Training model model_1.4\n",
            "200/200 [==============================] - 44s 194ms/step - loss: 0.9129 - acc: 0.5419 - val_loss: 0.6613 - val_acc: 0.6212\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.4.weights\n",
            "200/200 [==============================] - 38s 189ms/step - loss: 0.7038 - acc: 0.5967 - val_loss: 0.6109 - val_acc: 0.6450\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.4.weights\n",
            "200/200 [==============================] - 38s 190ms/step - loss: 0.6426 - acc: 0.6298 - val_loss: 0.6274 - val_acc: 0.6388\n",
            "200/200 [==============================] - 38s 189ms/step - loss: 0.6300 - acc: 0.6345 - val_loss: 0.5858 - val_acc: 0.6425\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.6172 - acc: 0.6436 - val_loss: 0.5807 - val_acc: 0.6737\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.4.weights\n",
            "200/200 [==============================] - 39s 194ms/step - loss: 0.5966 - acc: 0.6605 - val_loss: 0.5931 - val_acc: 0.6812\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.4.weights\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5947 - acc: 0.6706 - val_loss: 0.5642 - val_acc: 0.6963\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_1.4.weights\n",
            "200/200 [==============================] - 39s 197ms/step - loss: 0.6001 - acc: 0.6548 - val_loss: 0.5969 - val_acc: 0.6625\n",
            "200/200 [==============================] - 39s 194ms/step - loss: 0.5912 - acc: 0.6730 - val_loss: 0.6163 - val_acc: 0.6538\n",
            "200/200 [==============================] - 39s 195ms/step - loss: 0.5835 - acc: 0.6809 - val_loss: 0.5791 - val_acc: 0.6775\n",
            "200/200 [==============================] - 39s 195ms/step - loss: 0.5757 - acc: 0.6830 - val_loss: 0.5993 - val_acc: 0.6538\n",
            "200/200 [==============================] - 38s 193ms/step - loss: 0.5868 - acc: 0.6731 - val_loss: 0.6214 - val_acc: 0.6600\n",
            "200/200 [==============================] - 39s 197ms/step - loss: 0.5748 - acc: 0.6853 - val_loss: 0.5707 - val_acc: 0.6850\n",
            "200/200 [==============================] - 39s 196ms/step - loss: 0.5777 - acc: 0.6772 - val_loss: 0.5632 - val_acc: 0.6787\n",
            "200/200 [==============================] - 39s 194ms/step - loss: 0.5838 - acc: 0.6747 - val_loss: 0.6224 - val_acc: 0.6600\n",
            "reduced learning rate to 0.0001\n",
            "200/200 [==============================] - 45s 200ms/step - loss: 0.5816 - acc: 0.6762 - val_loss: 0.5612 - val_acc: 0.6762\n",
            "end training for model early\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2GkHcDuNF4m",
        "outputId": "7e38b2df-4316-4f7e-de55-52eb8e9f17f3"
      },
      "source": [
        "# train the same model multiples times based on different split of train and validation data\n",
        "import gc \n",
        "\n",
        "k_fold = 4\n",
        "random.shuffle(families)\n",
        "size_ = len(families) // k_fold\n",
        "groups = [[x, x + size_] for x in range(0, len(families), size_)]\n",
        "groups[-1][1] = len(families)\n",
        "\n",
        "model_factory = model_2\n",
        "model_name = \"model_2.\"\n",
        "model = None\n",
        "\n",
        "for i in range(0, k_fold): \n",
        "    val_start_index, val_end_index = groups[i]\n",
        "    val_relationships = families[val_start_index:val_end_index]\n",
        "    train_relationships = families[0:val_start_index] \\\n",
        "                        + families[val_end_index:len(families)]\n",
        "    del model\n",
        "    gc.collect()\n",
        "    model = model_factory()\n",
        "    train(model, model_name + str(i+1), model_max_acc_dict, \n",
        "          read_img, read_img, 0.001, 10, gen, \n",
        "          train_relationships, val_relationships)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model model_2.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 69s 225ms/step - loss: 0.8202 - acc: 0.5713 - val_loss: 0.6229 - val_acc: 0.6475\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.1.weights\n",
            "200/200 [==============================] - 41s 203ms/step - loss: 0.6549 - acc: 0.6308 - val_loss: 0.6179 - val_acc: 0.6325\n",
            "200/200 [==============================] - 39s 197ms/step - loss: 0.6226 - acc: 0.6455 - val_loss: 0.6250 - val_acc: 0.6288\n",
            "200/200 [==============================] - 39s 194ms/step - loss: 0.6023 - acc: 0.6641 - val_loss: 0.6088 - val_acc: 0.6375\n",
            "200/200 [==============================] - 38s 192ms/step - loss: 0.6052 - acc: 0.6550 - val_loss: 0.6134 - val_acc: 0.6538\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.1.weights\n",
            "200/200 [==============================] - 39s 193ms/step - loss: 0.5835 - acc: 0.6706 - val_loss: 0.6034 - val_acc: 0.6413\n",
            "200/200 [==============================] - 39s 193ms/step - loss: 0.5846 - acc: 0.6731 - val_loss: 0.6021 - val_acc: 0.6500\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5757 - acc: 0.6862 - val_loss: 0.5818 - val_acc: 0.6875\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.1.weights\n",
            "200/200 [==============================] - 39s 193ms/step - loss: 0.5770 - acc: 0.6888 - val_loss: 0.6014 - val_acc: 0.6438\n",
            "200/200 [==============================] - 39s 195ms/step - loss: 0.5716 - acc: 0.6819 - val_loss: 0.5942 - val_acc: 0.6725\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5655 - acc: 0.6908 - val_loss: 0.5924 - val_acc: 0.6625\n",
            "200/200 [==============================] - 38s 189ms/step - loss: 0.5727 - acc: 0.6873 - val_loss: 0.6095 - val_acc: 0.6625\n",
            "200/200 [==============================] - 39s 193ms/step - loss: 0.5661 - acc: 0.6916 - val_loss: 0.5783 - val_acc: 0.6837\n",
            "200/200 [==============================] - 39s 194ms/step - loss: 0.5654 - acc: 0.6964 - val_loss: 0.5864 - val_acc: 0.6837\n",
            "200/200 [==============================] - 38s 190ms/step - loss: 0.5553 - acc: 0.7033 - val_loss: 0.5941 - val_acc: 0.6562\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5613 - acc: 0.6980 - val_loss: 0.6147 - val_acc: 0.6538\n",
            "200/200 [==============================] - 38s 190ms/step - loss: 0.5629 - acc: 0.6889 - val_loss: 0.6140 - val_acc: 0.6413\n",
            "end training for model early\n",
            "Training model model_2.2\n",
            "200/200 [==============================] - 48s 201ms/step - loss: 0.8451 - acc: 0.5547 - val_loss: 0.6576 - val_acc: 0.6275\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.2.weights\n",
            "200/200 [==============================] - 39s 195ms/step - loss: 0.6770 - acc: 0.6134 - val_loss: 0.6061 - val_acc: 0.6600\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.2.weights\n",
            "200/200 [==============================] - 38s 192ms/step - loss: 0.6434 - acc: 0.6273 - val_loss: 0.6028 - val_acc: 0.6463\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.6228 - acc: 0.6433 - val_loss: 0.5684 - val_acc: 0.6900\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.2.weights\n",
            "200/200 [==============================] - 39s 194ms/step - loss: 0.5953 - acc: 0.6708 - val_loss: 0.5794 - val_acc: 0.6825\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5996 - acc: 0.6620 - val_loss: 0.5902 - val_acc: 0.6538\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5826 - acc: 0.6828 - val_loss: 0.5934 - val_acc: 0.6812\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5893 - acc: 0.6727 - val_loss: 0.5551 - val_acc: 0.7063\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.2.weights\n",
            "200/200 [==============================] - 39s 193ms/step - loss: 0.5808 - acc: 0.6823 - val_loss: 0.5683 - val_acc: 0.6725\n",
            "reduced learning rate to 0.0001\n",
            "200/200 [==============================] - 48s 201ms/step - loss: 0.5797 - acc: 0.6869 - val_loss: 0.5362 - val_acc: 0.7212\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.2.weights\n",
            "200/200 [==============================] - 39s 195ms/step - loss: 0.5688 - acc: 0.6880 - val_loss: 0.5501 - val_acc: 0.7075\n",
            "200/200 [==============================] - 38s 193ms/step - loss: 0.5642 - acc: 0.6958 - val_loss: 0.5834 - val_acc: 0.6612\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5674 - acc: 0.6916 - val_loss: 0.5695 - val_acc: 0.6913\n",
            "200/200 [==============================] - 38s 190ms/step - loss: 0.5670 - acc: 0.6906 - val_loss: 0.5655 - val_acc: 0.7075\n",
            "reduced learning rate to 1e-05\n",
            "200/200 [==============================] - 50s 210ms/step - loss: 0.5724 - acc: 0.6893 - val_loss: 0.5824 - val_acc: 0.6750\n",
            "reduced learning rate to 1.0000000000000002e-06\n",
            "200/200 [==============================] - 50s 209ms/step - loss: 0.5600 - acc: 0.7013 - val_loss: 0.5948 - val_acc: 0.6612\n",
            "200/200 [==============================] - 39s 197ms/step - loss: 0.5784 - acc: 0.6775 - val_loss: 0.5569 - val_acc: 0.6950\n",
            "200/200 [==============================] - 39s 193ms/step - loss: 0.5629 - acc: 0.7011 - val_loss: 0.5597 - val_acc: 0.7075\n",
            "200/200 [==============================] - 39s 193ms/step - loss: 0.5698 - acc: 0.6878 - val_loss: 0.5857 - val_acc: 0.6787\n",
            "end training for model early\n",
            "Training model model_2.3\n",
            "200/200 [==============================] - 48s 202ms/step - loss: 0.8640 - acc: 0.5490 - val_loss: 0.6682 - val_acc: 0.6263\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.3.weights\n",
            "200/200 [==============================] - 38s 193ms/step - loss: 0.6735 - acc: 0.6187 - val_loss: 0.6327 - val_acc: 0.6438\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.3.weights\n",
            "200/200 [==============================] - 39s 194ms/step - loss: 0.6410 - acc: 0.6323 - val_loss: 0.5895 - val_acc: 0.6625\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.3.weights\n",
            "200/200 [==============================] - 38s 192ms/step - loss: 0.6076 - acc: 0.6644 - val_loss: 0.5717 - val_acc: 0.6938\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.3.weights\n",
            "200/200 [==============================] - 39s 194ms/step - loss: 0.6054 - acc: 0.6672 - val_loss: 0.5712 - val_acc: 0.6825\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5972 - acc: 0.6678 - val_loss: 0.5807 - val_acc: 0.6700\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5987 - acc: 0.6598 - val_loss: 0.5775 - val_acc: 0.6538\n",
            "200/200 [==============================] - 38s 192ms/step - loss: 0.5939 - acc: 0.6719 - val_loss: 0.5595 - val_acc: 0.6862\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5845 - acc: 0.6745 - val_loss: 0.5649 - val_acc: 0.6988\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.3.weights\n",
            "200/200 [==============================] - 39s 193ms/step - loss: 0.5828 - acc: 0.6777 - val_loss: 0.5487 - val_acc: 0.7175\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.3.weights\n",
            "200/200 [==============================] - 39s 194ms/step - loss: 0.5871 - acc: 0.6761 - val_loss: 0.5731 - val_acc: 0.6750\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5791 - acc: 0.6773 - val_loss: 0.5839 - val_acc: 0.6538\n",
            "reduced learning rate to 0.0001\n",
            "200/200 [==============================] - 48s 202ms/step - loss: 0.5649 - acc: 0.6973 - val_loss: 0.5891 - val_acc: 0.6712\n",
            "200/200 [==============================] - 39s 196ms/step - loss: 0.5682 - acc: 0.6847 - val_loss: 0.5650 - val_acc: 0.6662\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5614 - acc: 0.6948 - val_loss: 0.5521 - val_acc: 0.6875\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5729 - acc: 0.6919 - val_loss: 0.5502 - val_acc: 0.6913\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5594 - acc: 0.6998 - val_loss: 0.5734 - val_acc: 0.6712\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5601 - acc: 0.6964 - val_loss: 0.5511 - val_acc: 0.7237\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.3.weights\n",
            "200/200 [==============================] - 38s 191ms/step - loss: 0.5565 - acc: 0.6947 - val_loss: 0.5203 - val_acc: 0.7275\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.3.weights\n",
            "reduced learning rate to 1e-05\n",
            "200/200 [==============================] - 48s 202ms/step - loss: 0.5599 - acc: 0.6971 - val_loss: 0.5529 - val_acc: 0.7000\n",
            "reduced learning rate to 1.0000000000000002e-06\n",
            "200/200 [==============================] - 48s 204ms/step - loss: 0.5686 - acc: 0.6897 - val_loss: 0.5509 - val_acc: 0.7163\n",
            "reduced learning rate to 1.0000000000000002e-07\n",
            "200/200 [==============================] - 50s 212ms/step - loss: 0.5582 - acc: 0.7023 - val_loss: 0.5635 - val_acc: 0.6812\n",
            "reduced learning rate to 1.0000000000000004e-08\n",
            "200/200 [==============================] - 49s 208ms/step - loss: 0.5647 - acc: 0.6984 - val_loss: 0.5700 - val_acc: 0.6913\n",
            "reduced learning rate to 1.0000000000000005e-09\n",
            "200/200 [==============================] - 49s 209ms/step - loss: 0.5671 - acc: 0.6840 - val_loss: 0.5517 - val_acc: 0.7138\n",
            "reduced learning rate to 1.0000000000000006e-10\n",
            "200/200 [==============================] - 50s 212ms/step - loss: 0.5718 - acc: 0.6848 - val_loss: 0.5864 - val_acc: 0.6550\n",
            "reduced learning rate to 1.0000000000000006e-11\n",
            "200/200 [==============================] - 49s 207ms/step - loss: 0.5564 - acc: 0.6935 - val_loss: 0.5634 - val_acc: 0.6888\n",
            "reduced learning rate to 1.0000000000000006e-12\n",
            "200/200 [==============================] - 50s 209ms/step - loss: 0.5580 - acc: 0.6995 - val_loss: 0.5632 - val_acc: 0.7038\n",
            "reduced learning rate to 1.0000000000000007e-13\n",
            "200/200 [==============================] - 49s 208ms/step - loss: 0.5571 - acc: 0.6926 - val_loss: 0.5662 - val_acc: 0.7013\n",
            "end training for model early\n",
            "Training model model_2.4\n",
            "200/200 [==============================] - 48s 204ms/step - loss: 0.8535 - acc: 0.5559 - val_loss: 0.7014 - val_acc: 0.5962\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.4.weights\n",
            "200/200 [==============================] - 39s 195ms/step - loss: 0.6717 - acc: 0.6164 - val_loss: 0.6251 - val_acc: 0.6562\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.4.weights\n",
            "200/200 [==============================] - 40s 200ms/step - loss: 0.6221 - acc: 0.6558 - val_loss: 0.5936 - val_acc: 0.6725\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.4.weights\n",
            "200/200 [==============================] - 39s 197ms/step - loss: 0.6164 - acc: 0.6492 - val_loss: 0.6213 - val_acc: 0.6288\n",
            "200/200 [==============================] - 39s 197ms/step - loss: 0.5992 - acc: 0.6634 - val_loss: 0.6013 - val_acc: 0.6625\n",
            "200/200 [==============================] - 40s 200ms/step - loss: 0.5937 - acc: 0.6661 - val_loss: 0.5856 - val_acc: 0.6538\n",
            "200/200 [==============================] - 40s 200ms/step - loss: 0.5924 - acc: 0.6727 - val_loss: 0.6374 - val_acc: 0.6400\n",
            "200/200 [==============================] - 39s 195ms/step - loss: 0.5841 - acc: 0.6800 - val_loss: 0.5690 - val_acc: 0.6800\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.4.weights\n",
            "200/200 [==============================] - 39s 197ms/step - loss: 0.5783 - acc: 0.6803 - val_loss: 0.5694 - val_acc: 0.6900\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_2.4.weights\n",
            "200/200 [==============================] - 39s 196ms/step - loss: 0.5811 - acc: 0.6783 - val_loss: 0.5805 - val_acc: 0.6712\n",
            "200/200 [==============================] - 39s 198ms/step - loss: 0.5757 - acc: 0.6848 - val_loss: 0.5799 - val_acc: 0.6650\n",
            "200/200 [==============================] - 40s 198ms/step - loss: 0.5626 - acc: 0.6969 - val_loss: 0.5922 - val_acc: 0.6637\n",
            "200/200 [==============================] - 40s 200ms/step - loss: 0.5738 - acc: 0.6870 - val_loss: 0.5860 - val_acc: 0.6787\n",
            "200/200 [==============================] - 40s 199ms/step - loss: 0.5694 - acc: 0.6958 - val_loss: 0.5952 - val_acc: 0.6538\n",
            "reduced learning rate to 0.0001\n",
            "200/200 [==============================] - 49s 207ms/step - loss: 0.5458 - acc: 0.7174 - val_loss: 0.5757 - val_acc: 0.6850\n",
            "200/200 [==============================] - 40s 199ms/step - loss: 0.5661 - acc: 0.6916 - val_loss: 0.5891 - val_acc: 0.6612\n",
            "200/200 [==============================] - 40s 199ms/step - loss: 0.5476 - acc: 0.6980 - val_loss: 0.5907 - val_acc: 0.6737\n",
            "reduced learning rate to 1e-05\n",
            "200/200 [==============================] - 50s 211ms/step - loss: 0.5680 - acc: 0.6932 - val_loss: 0.6012 - val_acc: 0.6812\n",
            "end training for model early\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV7xHdYAB8qF",
        "outputId": "377cf85e-5144-41fe-d1d0-e07c6c2ff4ea"
      },
      "source": [
        "# train the same model multiples times based on different split of train and validation data\n",
        "import gc\n",
        "\n",
        "k_fold = 4\n",
        "\n",
        "size_ = len(families) // k_fold\n",
        "groups = [[x, x + size_] for x in range(0, len(families), size_)]\n",
        "groups[-1][1] = len(families)\n",
        "\n",
        "model_factory = model_3\n",
        "model_name = \"model_3.\"\n",
        "model = None\n",
        "\n",
        "for i in range(0, k_fold): \n",
        "    val_start_index, val_end_index = groups[i]\n",
        "    val_relationships = families[val_start_index:val_end_index]\n",
        "    train_relationships = families[0:val_start_index] \\\n",
        "                        + families[val_end_index:len(families)]\n",
        "    del model\n",
        "    gc.collect()\n",
        "    model = model_factory()\n",
        "    train(model, model_name + str(i+1), model_max_acc_dict, \n",
        "          read_img, read_img, 0.001, 10, gen, \n",
        "          train_relationships, val_relationships)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_resnet50.h5\n",
            "94699520/94694792 [==============================] - 1s 0us/step\n",
            "Training model model_3.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 2893s 14s/step - loss: 0.8958 - acc: 0.5143 - val_loss: 0.6588 - val_acc: 0.5962\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.1.weights\n",
            "200/200 [==============================] - 1515s 8s/step - loss: 0.7396 - acc: 0.5475 - val_loss: 0.6270 - val_acc: 0.6300\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.1.weights\n",
            "200/200 [==============================] - 882s 4s/step - loss: 0.6731 - acc: 0.5916 - val_loss: 0.6201 - val_acc: 0.6250\n",
            "200/200 [==============================] - 563s 3s/step - loss: 0.6542 - acc: 0.6097 - val_loss: 0.6214 - val_acc: 0.6250\n",
            "200/200 [==============================] - 381s 2s/step - loss: 0.6377 - acc: 0.6227 - val_loss: 0.6163 - val_acc: 0.6438\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.1.weights\n",
            "200/200 [==============================] - 268s 1s/step - loss: 0.6313 - acc: 0.6319 - val_loss: 0.6024 - val_acc: 0.6637\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.1.weights\n",
            "200/200 [==============================] - 218s 1s/step - loss: 0.6143 - acc: 0.6470 - val_loss: 0.5964 - val_acc: 0.6700\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.1.weights\n",
            "200/200 [==============================] - 165s 829ms/step - loss: 0.6071 - acc: 0.6550 - val_loss: 0.5761 - val_acc: 0.6862\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.1.weights\n",
            "200/200 [==============================] - 147s 738ms/step - loss: 0.6016 - acc: 0.6544 - val_loss: 0.5772 - val_acc: 0.7013\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.1.weights\n",
            "200/200 [==============================] - 119s 597ms/step - loss: 0.6084 - acc: 0.6536 - val_loss: 0.5998 - val_acc: 0.6762\n",
            "reduced learning rate to 0.0001\n",
            "200/200 [==============================] - 105s 502ms/step - loss: 0.5816 - acc: 0.6750 - val_loss: 0.5737 - val_acc: 0.6950\n",
            "200/200 [==============================] - 97s 486ms/step - loss: 0.5949 - acc: 0.6623 - val_loss: 0.5547 - val_acc: 0.6938\n",
            "200/200 [==============================] - 85s 425ms/step - loss: 0.5964 - acc: 0.6600 - val_loss: 0.5548 - val_acc: 0.7100\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.1.weights\n",
            "reduced learning rate to 1e-05\n",
            "200/200 [==============================] - 84s 398ms/step - loss: 0.5878 - acc: 0.6839 - val_loss: 0.5806 - val_acc: 0.6612\n",
            "200/200 [==============================] - 73s 367ms/step - loss: 0.5944 - acc: 0.6709 - val_loss: 0.6121 - val_acc: 0.6225\n",
            "200/200 [==============================] - 65s 327ms/step - loss: 0.5834 - acc: 0.6764 - val_loss: 0.5587 - val_acc: 0.6925\n",
            "reduced learning rate to 1.0000000000000002e-06\n",
            "200/200 [==============================] - 65s 298ms/step - loss: 0.5882 - acc: 0.6573 - val_loss: 0.5725 - val_acc: 0.6825\n",
            "reduced learning rate to 1.0000000000000002e-07\n",
            "200/200 [==============================] - 63s 291ms/step - loss: 0.5961 - acc: 0.6730 - val_loss: 0.5903 - val_acc: 0.6637\n",
            "reduced learning rate to 1.0000000000000004e-08\n",
            "200/200 [==============================] - 60s 276ms/step - loss: 0.5995 - acc: 0.6662 - val_loss: 0.5816 - val_acc: 0.6562\n",
            "reduced learning rate to 1.0000000000000005e-09\n",
            "200/200 [==============================] - 58s 263ms/step - loss: 0.5936 - acc: 0.6707 - val_loss: 0.5645 - val_acc: 0.6925\n",
            "reduced learning rate to 1.0000000000000006e-10\n",
            "200/200 [==============================] - 61s 283ms/step - loss: 0.5882 - acc: 0.6707 - val_loss: 0.5857 - val_acc: 0.6812\n",
            "reduced learning rate to 1.0000000000000006e-11\n",
            "200/200 [==============================] - 60s 275ms/step - loss: 0.5924 - acc: 0.6627 - val_loss: 0.5981 - val_acc: 0.6488\n",
            "end training for model early\n",
            "Training model model_3.2\n",
            "200/200 [==============================] - 65s 298ms/step - loss: 0.8653 - acc: 0.5217 - val_loss: 0.6633 - val_acc: 0.5838\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.2.weights\n",
            "200/200 [==============================] - 55s 275ms/step - loss: 0.7193 - acc: 0.5638 - val_loss: 0.6340 - val_acc: 0.6075\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.2.weights\n",
            "200/200 [==============================] - 56s 280ms/step - loss: 0.6795 - acc: 0.5944 - val_loss: 0.6380 - val_acc: 0.6275\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.2.weights\n",
            "200/200 [==============================] - 52s 259ms/step - loss: 0.6546 - acc: 0.6100 - val_loss: 0.6565 - val_acc: 0.6087\n",
            "200/200 [==============================] - 48s 240ms/step - loss: 0.6234 - acc: 0.6383 - val_loss: 0.5911 - val_acc: 0.6500\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.2.weights\n",
            "200/200 [==============================] - 48s 242ms/step - loss: 0.6197 - acc: 0.6450 - val_loss: 0.6305 - val_acc: 0.6212\n",
            "200/200 [==============================] - 49s 245ms/step - loss: 0.6110 - acc: 0.6492 - val_loss: 0.6345 - val_acc: 0.6237\n",
            "200/200 [==============================] - 49s 245ms/step - loss: 0.6004 - acc: 0.6608 - val_loss: 0.6099 - val_acc: 0.6313\n",
            "200/200 [==============================] - 48s 240ms/step - loss: 0.6026 - acc: 0.6595 - val_loss: 0.6063 - val_acc: 0.6575\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.2.weights\n",
            "200/200 [==============================] - 48s 241ms/step - loss: 0.5978 - acc: 0.6598 - val_loss: 0.6043 - val_acc: 0.6538\n",
            "reduced learning rate to 0.0001\n",
            "200/200 [==============================] - 51s 232ms/step - loss: 0.5987 - acc: 0.6604 - val_loss: 0.5727 - val_acc: 0.6837\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.2.weights\n",
            "200/200 [==============================] - 46s 229ms/step - loss: 0.5958 - acc: 0.6669 - val_loss: 0.5935 - val_acc: 0.6687\n",
            "200/200 [==============================] - 45s 228ms/step - loss: 0.5853 - acc: 0.6695 - val_loss: 0.6184 - val_acc: 0.6488\n",
            "200/200 [==============================] - 45s 223ms/step - loss: 0.5921 - acc: 0.6681 - val_loss: 0.5919 - val_acc: 0.6600\n",
            "200/200 [==============================] - 44s 221ms/step - loss: 0.5859 - acc: 0.6812 - val_loss: 0.5781 - val_acc: 0.6700\n",
            "200/200 [==============================] - 43s 218ms/step - loss: 0.5787 - acc: 0.6808 - val_loss: 0.5889 - val_acc: 0.6687\n",
            "200/200 [==============================] - 44s 219ms/step - loss: 0.5749 - acc: 0.6844 - val_loss: 0.6060 - val_acc: 0.6625\n",
            "200/200 [==============================] - 45s 224ms/step - loss: 0.5855 - acc: 0.6775 - val_loss: 0.5776 - val_acc: 0.6862\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.2.weights\n",
            "200/200 [==============================] - 44s 222ms/step - loss: 0.5762 - acc: 0.6825 - val_loss: 0.6078 - val_acc: 0.6650\n",
            "reduced learning rate to 1e-05\n",
            "200/200 [==============================] - 51s 228ms/step - loss: 0.5729 - acc: 0.6927 - val_loss: 0.5535 - val_acc: 0.7000\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.2.weights\n",
            "200/200 [==============================] - 45s 225ms/step - loss: 0.5813 - acc: 0.6783 - val_loss: 0.6083 - val_acc: 0.6300\n",
            "200/200 [==============================] - 43s 214ms/step - loss: 0.5749 - acc: 0.6820 - val_loss: 0.5797 - val_acc: 0.6600\n",
            "reduced learning rate to 1.0000000000000002e-06\n",
            "200/200 [==============================] - 50s 226ms/step - loss: 0.5722 - acc: 0.6875 - val_loss: 0.5743 - val_acc: 0.6538\n",
            "reduced learning rate to 1.0000000000000002e-07\n",
            "200/200 [==============================] - 51s 229ms/step - loss: 0.5762 - acc: 0.6812 - val_loss: 0.5809 - val_acc: 0.6700\n",
            "reduced learning rate to 1.0000000000000004e-08\n",
            "200/200 [==============================] - 51s 230ms/step - loss: 0.5791 - acc: 0.6818 - val_loss: 0.5664 - val_acc: 0.6888\n",
            "reduced learning rate to 1.0000000000000005e-09\n",
            "200/200 [==============================] - 51s 229ms/step - loss: 0.5736 - acc: 0.6833 - val_loss: 0.5865 - val_acc: 0.6650\n",
            "reduced learning rate to 1.0000000000000006e-10\n",
            "200/200 [==============================] - 48s 214ms/step - loss: 0.5745 - acc: 0.6794 - val_loss: 0.5828 - val_acc: 0.6925\n",
            "reduced learning rate to 1.0000000000000006e-11\n",
            "200/200 [==============================] - 51s 230ms/step - loss: 0.5701 - acc: 0.6930 - val_loss: 0.5821 - val_acc: 0.6637\n",
            "200/200 [==============================] - 44s 222ms/step - loss: 0.5850 - acc: 0.6828 - val_loss: 0.5737 - val_acc: 0.6637\n",
            "end training for model early\n",
            "Training model model_3.3\n",
            "200/200 [==============================] - 52s 233ms/step - loss: 0.8779 - acc: 0.5237 - val_loss: 0.6636 - val_acc: 0.5713\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.3.weights\n",
            "200/200 [==============================] - 44s 219ms/step - loss: 0.7257 - acc: 0.5670 - val_loss: 0.6502 - val_acc: 0.5962\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.3.weights\n",
            "200/200 [==============================] - 43s 216ms/step - loss: 0.6781 - acc: 0.5958 - val_loss: 0.6174 - val_acc: 0.6300\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.3.weights\n",
            "200/200 [==============================] - 45s 223ms/step - loss: 0.6522 - acc: 0.6112 - val_loss: 0.6439 - val_acc: 0.6200\n",
            "200/200 [==============================] - 43s 213ms/step - loss: 0.6319 - acc: 0.6308 - val_loss: 0.6182 - val_acc: 0.6513\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.3.weights\n",
            "200/200 [==============================] - 42s 211ms/step - loss: 0.6103 - acc: 0.6489 - val_loss: 0.6188 - val_acc: 0.6513\n",
            "200/200 [==============================] - 43s 217ms/step - loss: 0.6017 - acc: 0.6602 - val_loss: 0.6077 - val_acc: 0.6388\n",
            "200/200 [==============================] - 44s 218ms/step - loss: 0.5990 - acc: 0.6538 - val_loss: 0.6266 - val_acc: 0.6325\n",
            "200/200 [==============================] - 43s 214ms/step - loss: 0.5957 - acc: 0.6636 - val_loss: 0.6090 - val_acc: 0.6500\n",
            "200/200 [==============================] - 44s 222ms/step - loss: 0.5925 - acc: 0.6637 - val_loss: 0.6043 - val_acc: 0.6500\n",
            "200/200 [==============================] - 43s 214ms/step - loss: 0.5828 - acc: 0.6744 - val_loss: 0.6171 - val_acc: 0.6525\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.3.weights\n",
            "200/200 [==============================] - 42s 212ms/step - loss: 0.5864 - acc: 0.6733 - val_loss: 0.6035 - val_acc: 0.6712\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.3.weights\n",
            "200/200 [==============================] - 45s 224ms/step - loss: 0.5918 - acc: 0.6578 - val_loss: 0.5895 - val_acc: 0.6762\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.3.weights\n",
            "reduced learning rate to 0.0001\n",
            "200/200 [==============================] - 49s 221ms/step - loss: 0.5949 - acc: 0.6640 - val_loss: 0.6148 - val_acc: 0.6400\n",
            "reduced learning rate to 1e-05\n",
            "200/200 [==============================] - 52s 234ms/step - loss: 0.5869 - acc: 0.6696 - val_loss: 0.5916 - val_acc: 0.6725\n",
            "reduced learning rate to 1.0000000000000002e-06\n",
            "200/200 [==============================] - 49s 220ms/step - loss: 0.5848 - acc: 0.6737 - val_loss: 0.6128 - val_acc: 0.6662\n",
            "200/200 [==============================] - 42s 210ms/step - loss: 0.5726 - acc: 0.6853 - val_loss: 0.6204 - val_acc: 0.6325\n",
            "200/200 [==============================] - 43s 216ms/step - loss: 0.5747 - acc: 0.6850 - val_loss: 0.6081 - val_acc: 0.6488\n",
            "200/200 [==============================] - 43s 214ms/step - loss: 0.5795 - acc: 0.6820 - val_loss: 0.6203 - val_acc: 0.6475\n",
            "reduced learning rate to 1.0000000000000002e-07\n",
            "200/200 [==============================] - 49s 221ms/step - loss: 0.5762 - acc: 0.6853 - val_loss: 0.6003 - val_acc: 0.6550\n",
            "reduced learning rate to 1.0000000000000004e-08\n",
            "200/200 [==============================] - 49s 223ms/step - loss: 0.5787 - acc: 0.6675 - val_loss: 0.5959 - val_acc: 0.6550\n",
            "reduced learning rate to 1.0000000000000005e-09\n",
            "200/200 [==============================] - 49s 219ms/step - loss: 0.5832 - acc: 0.6759 - val_loss: 0.5851 - val_acc: 0.6750\n",
            "end training for model early\n",
            "Training model model_3.4\n",
            "200/200 [==============================] - 51s 227ms/step - loss: 0.9176 - acc: 0.4912 - val_loss: 0.6586 - val_acc: 0.5838\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.4.weights\n",
            "200/200 [==============================] - 43s 215ms/step - loss: 0.7363 - acc: 0.5477 - val_loss: 0.6183 - val_acc: 0.6500\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.4.weights\n",
            "200/200 [==============================] - 44s 219ms/step - loss: 0.6886 - acc: 0.5827 - val_loss: 0.6067 - val_acc: 0.6463\n",
            "200/200 [==============================] - 43s 216ms/step - loss: 0.6661 - acc: 0.6019 - val_loss: 0.5809 - val_acc: 0.6825\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.4.weights\n",
            "200/200 [==============================] - 44s 220ms/step - loss: 0.6389 - acc: 0.6236 - val_loss: 0.6013 - val_acc: 0.6575\n",
            "200/200 [==============================] - 43s 215ms/step - loss: 0.6373 - acc: 0.6248 - val_loss: 0.5602 - val_acc: 0.6825\n",
            "200/200 [==============================] - 44s 220ms/step - loss: 0.6185 - acc: 0.6442 - val_loss: 0.5681 - val_acc: 0.6900\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.4.weights\n",
            "200/200 [==============================] - 44s 220ms/step - loss: 0.6127 - acc: 0.6492 - val_loss: 0.5856 - val_acc: 0.6812\n",
            "200/200 [==============================] - 42s 209ms/step - loss: 0.6097 - acc: 0.6491 - val_loss: 0.5884 - val_acc: 0.6675\n",
            "200/200 [==============================] - 42s 212ms/step - loss: 0.5988 - acc: 0.6614 - val_loss: 0.5497 - val_acc: 0.7025\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_3.4.weights\n",
            "200/200 [==============================] - 43s 213ms/step - loss: 0.6074 - acc: 0.6495 - val_loss: 0.5822 - val_acc: 0.6562\n",
            "200/200 [==============================] - 43s 216ms/step - loss: 0.5936 - acc: 0.6667 - val_loss: 0.5770 - val_acc: 0.6775\n",
            "200/200 [==============================] - 43s 218ms/step - loss: 0.5973 - acc: 0.6645 - val_loss: 0.5916 - val_acc: 0.6538\n",
            "200/200 [==============================] - 44s 220ms/step - loss: 0.6004 - acc: 0.6705 - val_loss: 0.5904 - val_acc: 0.6575\n",
            "200/200 [==============================] - 44s 221ms/step - loss: 0.5916 - acc: 0.6673 - val_loss: 0.5677 - val_acc: 0.6762\n",
            "200/200 [==============================] - 42s 212ms/step - loss: 0.5933 - acc: 0.6683 - val_loss: 0.5816 - val_acc: 0.6687\n",
            "reduced learning rate to 0.0001\n",
            "200/200 [==============================] - 49s 219ms/step - loss: 0.6006 - acc: 0.6613 - val_loss: 0.5772 - val_acc: 0.6737\n",
            "200/200 [==============================] - 43s 214ms/step - loss: 0.5866 - acc: 0.6734 - val_loss: 0.5657 - val_acc: 0.6875\n",
            "200/200 [==============================] - 42s 212ms/step - loss: 0.5801 - acc: 0.6816 - val_loss: 0.5797 - val_acc: 0.6825\n",
            "end training for model early\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE2mo_GYCHVr",
        "outputId": "bb8b322d-d52d-41cd-8845-85da55fdd1e1"
      },
      "source": [
        "# train the same model multiples times based on different split of train and validation data\n",
        "import gc\n",
        "\n",
        "k_fold = 4\n",
        "\n",
        "size_ = len(families) // k_fold\n",
        "groups = [[x, x + size_] for x in range(0, len(families), size_)]\n",
        "groups[-1][1] = len(families)\n",
        "\n",
        "model_factory = model_4\n",
        "model_name = \"model_4.\"\n",
        "model = None\n",
        "\n",
        "for i in range(0, k_fold): \n",
        "    val_start_index, val_end_index = groups[i]\n",
        "    val_relationships = families[val_start_index:val_end_index]\n",
        "    train_relationships = families[0:val_start_index] \\\n",
        "                        + families[val_end_index:len(families)]\n",
        "    del model\n",
        "    gc.collect()\n",
        "    model = model_factory()\n",
        "    train(model, model_name + str(i+1), model_max_acc_dict, \n",
        "          read_img, read_img, 0.001, 5, gen, \n",
        "          train_relationships, val_relationships)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model model_4.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 77s 256ms/step - loss: 0.8737 - acc: 0.5127 - val_loss: 0.6276 - val_acc: 0.6488\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.1.weights\n",
            "200/200 [==============================] - 46s 228ms/step - loss: 0.7052 - acc: 0.5855 - val_loss: 0.6357 - val_acc: 0.6325\n",
            "200/200 [==============================] - 45s 224ms/step - loss: 0.6637 - acc: 0.6106 - val_loss: 0.6109 - val_acc: 0.6275\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.6392 - acc: 0.6212 - val_loss: 0.5938 - val_acc: 0.6900\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.1.weights\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.6247 - acc: 0.6322 - val_loss: 0.5784 - val_acc: 0.6488\n",
            "200/200 [==============================] - 45s 225ms/step - loss: 0.6130 - acc: 0.6420 - val_loss: 0.5629 - val_acc: 0.7138\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.1.weights\n",
            "200/200 [==============================] - 46s 230ms/step - loss: 0.6137 - acc: 0.6427 - val_loss: 0.5836 - val_acc: 0.6750\n",
            "200/200 [==============================] - 45s 225ms/step - loss: 0.5955 - acc: 0.6703 - val_loss: 0.5713 - val_acc: 0.6888\n",
            "200/200 [==============================] - 45s 227ms/step - loss: 0.5972 - acc: 0.6631 - val_loss: 0.5872 - val_acc: 0.6650\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.5905 - acc: 0.6714 - val_loss: 0.5686 - val_acc: 0.7088\n",
            "end training for model early\n",
            "Training model model_4.2\n",
            "200/200 [==============================] - 56s 237ms/step - loss: 0.8952 - acc: 0.5158 - val_loss: 0.6777 - val_acc: 0.5625\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.2.weights\n",
            "200/200 [==============================] - 46s 228ms/step - loss: 0.7217 - acc: 0.5698 - val_loss: 0.6324 - val_acc: 0.6463\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.2.weights\n",
            "200/200 [==============================] - 45s 227ms/step - loss: 0.6766 - acc: 0.5969 - val_loss: 0.6262 - val_acc: 0.6363\n",
            "200/200 [==============================] - 45s 225ms/step - loss: 0.6314 - acc: 0.6334 - val_loss: 0.6035 - val_acc: 0.6400\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.6243 - acc: 0.6388 - val_loss: 0.5959 - val_acc: 0.6712\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.2.weights\n",
            "200/200 [==============================] - 46s 228ms/step - loss: 0.6182 - acc: 0.6403 - val_loss: 0.5944 - val_acc: 0.6600\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.6065 - acc: 0.6630 - val_loss: 0.5670 - val_acc: 0.7088\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.2.weights\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.5855 - acc: 0.6672 - val_loss: 0.5911 - val_acc: 0.6762\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.5874 - acc: 0.6677 - val_loss: 0.5732 - val_acc: 0.6825\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.5845 - acc: 0.6755 - val_loss: 0.6144 - val_acc: 0.6687\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.5854 - acc: 0.6716 - val_loss: 0.5812 - val_acc: 0.6913\n",
            "end training for model early\n",
            "Training model model_4.3\n",
            "200/200 [==============================] - 56s 237ms/step - loss: 0.9057 - acc: 0.5135 - val_loss: 0.6876 - val_acc: 0.5813\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.3.weights\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.7282 - acc: 0.5725 - val_loss: 0.6583 - val_acc: 0.5962\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.3.weights\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.6755 - acc: 0.6047 - val_loss: 0.6411 - val_acc: 0.6125\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.3.weights\n",
            "200/200 [==============================] - 45s 227ms/step - loss: 0.6378 - acc: 0.6298 - val_loss: 0.6201 - val_acc: 0.6375\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.3.weights\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.6279 - acc: 0.6394 - val_loss: 0.5861 - val_acc: 0.6575\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.3.weights\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.6175 - acc: 0.6463 - val_loss: 0.5913 - val_acc: 0.6600\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.3.weights\n",
            "200/200 [==============================] - 46s 229ms/step - loss: 0.6005 - acc: 0.6667 - val_loss: 0.6062 - val_acc: 0.6438\n",
            "200/200 [==============================] - 45s 227ms/step - loss: 0.5911 - acc: 0.6712 - val_loss: 0.5963 - val_acc: 0.6612\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.3.weights\n",
            "200/200 [==============================] - 45s 227ms/step - loss: 0.5927 - acc: 0.6691 - val_loss: 0.5797 - val_acc: 0.6812\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.3.weights\n",
            "200/200 [==============================] - 45s 227ms/step - loss: 0.5981 - acc: 0.6709 - val_loss: 0.5673 - val_acc: 0.6750\n",
            "reduced learning rate to 0.0001\n",
            "200/200 [==============================] - 55s 235ms/step - loss: 0.5787 - acc: 0.6880 - val_loss: 0.5657 - val_acc: 0.6913\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.3.weights\n",
            "200/200 [==============================] - 45s 227ms/step - loss: 0.5819 - acc: 0.6833 - val_loss: 0.5649 - val_acc: 0.6900\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.5828 - acc: 0.6748 - val_loss: 0.5675 - val_acc: 0.6800\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.5845 - acc: 0.6808 - val_loss: 0.5845 - val_acc: 0.6587\n",
            "reduced learning rate to 1e-05\n",
            "200/200 [==============================] - 55s 236ms/step - loss: 0.5818 - acc: 0.6769 - val_loss: 0.5805 - val_acc: 0.6562\n",
            "end training for model early\n",
            "Training model model_4.4\n",
            "200/200 [==============================] - 57s 240ms/step - loss: 0.8712 - acc: 0.5213 - val_loss: 0.6628 - val_acc: 0.5813\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.4.weights\n",
            "200/200 [==============================] - 45s 225ms/step - loss: 0.7213 - acc: 0.5811 - val_loss: 0.6372 - val_acc: 0.6363\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.4.weights\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.6738 - acc: 0.6053 - val_loss: 0.6225 - val_acc: 0.6687\n",
            "saved weights for model at /gdrive/MyDrive/FG 2021 RFIW2021/models/model_4.4.weights\n",
            "200/200 [==============================] - 46s 229ms/step - loss: 0.6369 - acc: 0.6386 - val_loss: 0.6169 - val_acc: 0.6550\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.6186 - acc: 0.6484 - val_loss: 0.6246 - val_acc: 0.6187\n",
            "200/200 [==============================] - 45s 225ms/step - loss: 0.6087 - acc: 0.6500 - val_loss: 0.6053 - val_acc: 0.6463\n",
            "200/200 [==============================] - 49s 246ms/step - loss: 0.5959 - acc: 0.6639 - val_loss: 0.5846 - val_acc: 0.6612\n",
            "end training for model early\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oUfagJxQOzi",
        "outputId": "f6965263-aca2-4b35-cd32-bed6d0461a6a"
      },
      "source": [
        "import json\n",
        "from pprint import pprint as pp\n",
        "model_max_acc_dict = {}\n",
        "with open('./models/model_max_acc_dict.json', 'r') as openfile:\n",
        "    model_max_acc_dict = json.load(openfile)\n",
        "    pp(model_max_acc_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'model_1.1': 0.6812499761581421,\n",
            " 'model_1.2': 0.699999988079071,\n",
            " 'model_1.3': 0.7174999713897705,\n",
            " 'model_1.4': 0.6962500214576721,\n",
            " 'model_2.1': 0.6875,\n",
            " 'model_2.2': 0.7212499976158142,\n",
            " 'model_2.3': 0.7275000214576721,\n",
            " 'model_2.4': 0.6899999976158142,\n",
            " 'model_3.1': 0.7099999785423279,\n",
            " 'model_3.2': 0.699999988079071,\n",
            " 'model_3.3': 0.6762499809265137,\n",
            " 'model_3.4': 0.7024999856948853,\n",
            " 'model_4.1': 0.7137500047683716,\n",
            " 'model_4.2': 0.7087500095367432,\n",
            " 'model_4.3': 0.6912500262260437,\n",
            " 'model_4.4': 0.668749988079071}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_fNoQSWmpdw"
      },
      "source": [
        "model_func_dict = {\n",
        " 'model_1.1': model_1,\n",
        " 'model_1.2': model_1,\n",
        " 'model_1.3': model_1,\n",
        " 'model_1.4': model_1,\n",
        " 'model_2.1': model_2,\n",
        " 'model_2.2': model_2,\n",
        " 'model_2.3': model_2,\n",
        " 'model_2.4': model_2,\n",
        " 'model_3.1': model_3,\n",
        " 'model_3.2': model_3,\n",
        " 'model_3.3': model_3,\n",
        " 'model_3.4': model_3,\n",
        " 'model_4.1': model_4,\n",
        " 'model_4.2': model_4,\n",
        " 'model_4.3': model_4,\n",
        " 'model_4.4': model_4,\n",
        " }\n",
        "\n",
        "model_weight_paths = {\n",
        " 'model_1.1': \"./models/model_1.1.weights.npy\",\n",
        " 'model_1.2': \"./models/model_1.2.weights.npy\",\n",
        " 'model_1.3': \"./models/model_1.3.weights.npy\",\n",
        " 'model_1.4': \"./models/model_1.4.weights.npy\",\n",
        " 'model_2.1': \"./models/model_2.1.weights.npy\",\n",
        " 'model_2.2': \"./models/model_2.2.weights.npy\",\n",
        " 'model_2.3': \"./models/model_2.3.weights.npy\",\n",
        " 'model_2.4': \"./models/model_2.4.weights.npy\",\n",
        " 'model_3.1': \"./models/model_3.1.weights.npy\",\n",
        " 'model_3.2': \"./models/model_3.2.weights.npy\",\n",
        " 'model_3.3': \"./models/model_3.3.weights.npy\",\n",
        " 'model_3.4': \"./models/model_3.4.weights.npy\",\n",
        " 'model_4.1': \"./models/model_4.1.weights.npy\",\n",
        " 'model_4.2': \"./models/model_4.2.weights.npy\",\n",
        " 'model_4.3': \"./models/model_4.3.weights.npy\",\n",
        " 'model_4.4': \"./models/model_4.4.weights.npy\",\n",
        " }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zkd_S-Z5cy9R"
      },
      "source": [
        "def get_models(weight_by_acc=True, filter_threshold=1):\n",
        "    models = []\n",
        "\n",
        "    if weight_by_acc: \n",
        "        for model_name, weight_file_path in model_weight_paths.items(): \n",
        "            acc = model_max_acc_dict[model_name]\n",
        "            print(\"loading \", model_name, \", val acc is \", acc)\n",
        "            if acc < filter_threshold: \n",
        "                print(\"\\tval acc is lower than \", str(filter_threshold), \", skipping\")\n",
        "                continue\n",
        "\n",
        "            model = model_func_dict[model_name]()\n",
        "            with open(weight_file_path, 'rb') as f:\n",
        "                weights = np.load(f, allow_pickle=True)\n",
        "                model.set_weights(weights) \n",
        "                models.append((model, acc))\n",
        "\n",
        "    else: # weight each model equally\n",
        "        for model_name, weight_file_path in model_weight_paths.items(): \n",
        "            print(\"loading \", model_name, \" with equal weight\")\n",
        "            acc = model_max_acc_dict[model_name]\n",
        "            if acc < filter_threshold: \n",
        "                print(\"\\tval acc is lower than \", str(filter_threshold), \", skipping\")\n",
        "\n",
        "            model = model_func_dict[model_name]()\n",
        "            with open(weight_file_path, 'rb') as f:\n",
        "                weights = np.load(f, allow_pickle=True)\n",
        "                model.set_weights(weights) \n",
        "                models.append((model, 1))\n",
        "\n",
        "    return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYLCmTCq8GRD",
        "outputId": "e1e8ce5f-78e6-4278-ce04-86d9e96d6c58"
      },
      "source": [
        "models = get_models(weight_by_acc=False, filter_threshold=0)\n",
        "# normalize weights of models\n",
        "sum_acc = sum([x[1] for x in models])\n",
        "models = [(x[0], x[1]/sum_acc) for x in models]\n",
        "pp(models)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading  model_1.1  with equal weight\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-09-15 01:38:26.507674: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-09-15 01:38:26.507950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:38:26.508663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-09-15 01:38:26.508710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-09-15 01:38:26.508771: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-09-15 01:38:26.508786: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-09-15 01:38:26.508800: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-09-15 01:38:26.508815: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-09-15 01:38:26.508829: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-09-15 01:38:26.508843: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-09-15 01:38:26.508858: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-09-15 01:38:26.508938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:38:26.509641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:38:26.510218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-09-15 01:38:26.510435: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-09-15 01:38:26.510557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:38:26.511159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-09-15 01:38:26.511180: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-09-15 01:38:26.511212: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-09-15 01:38:26.511223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-09-15 01:38:26.511235: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-09-15 01:38:26.511245: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-09-15 01:38:26.511256: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-09-15 01:38:26.511267: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-09-15 01:38:26.511278: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-09-15 01:38:26.511334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:38:26.511934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:38:26.512467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-09-15 01:38:26.512501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-09-15 01:38:26.512506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-09-15 01:38:26.512511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-09-15 01:38:26.512590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:38:26.513234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 01:38:26.513799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13968 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading  model_1.2  with equal weight\n",
            "loading  model_1.3  with equal weight\n",
            "loading  model_1.4  with equal weight\n",
            "loading  model_2.1  with equal weight\n",
            "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_senet50.h5\n",
            "104947712/104944616 [==============================] - 2s 0us/step\n",
            "loading  model_2.2  with equal weight\n",
            "loading  model_2.3  with equal weight\n",
            "loading  model_2.4  with equal weight\n",
            "loading  model_3.1  with equal weight\n",
            "loading  model_3.2  with equal weight\n",
            "loading  model_3.3  with equal weight\n",
            "loading  model_3.4  with equal weight\n",
            "loading  model_4.1  with equal weight\n",
            "loading  model_4.2  with equal weight\n",
            "loading  model_4.3  with equal weight\n",
            "loading  model_4.4  with equal weight\n",
            "[(<tensorflow.python.keras.engine.functional.Functional object at 0x7f53c40ca070>,\n",
            "  0.0625),\n",
            " (<tensorflow.python.keras.engine.functional.Functional object at 0x7f53c053d9a0>,\n",
            "  0.0625),\n",
            " (<tensorflow.python.keras.engine.functional.Functional object at 0x7f53c02133d0>,\n",
            "  0.0625),\n",
            " (<tensorflow.python.keras.engine.functional.Functional object at 0x7f53c0603550>,\n",
            "  0.0625),\n",
            " (<tensorflow.python.keras.engine.functional.Functional object at 0x7f5369597610>,\n",
            "  0.0625),\n",
            " (<tensorflow.python.keras.engine.functional.Functional object at 0x7f53696b6430>,\n",
            "  0.0625),\n",
            " (<tensorflow.python.keras.engine.functional.Functional object at 0x7f5368c92970>,\n",
            "  0.0625),\n",
            " (<tensorflow.python.keras.engine.functional.Functional object at 0x7f5369852e50>,\n",
            "  0.0625),\n",
            " (<tensorflow.python.keras.engine.functional.Functional object at 0x7f53688b9550>,\n",
            "  0.0625),\n",
            " (<tensorflow.python.keras.engine.functional.Functional object at 0x7f5368619f10>,\n",
            "  0.0625),\n",
            " (<tensorflow.python.keras.engine.functional.Functional object at 0x7f5369050130>,\n",
            "  0.0625),\n",
            " (<tensorflow.python.keras.engine.functional.Functional object at 0x7f5368169d60>,\n",
            "  0.0625),\n",
            " (<tensorflow.python.keras.engine.functional.Functional object at 0x7f5368a12c70>,\n",
            "  0.0625),\n",
            " (<tensorflow.python.keras.engine.functional.Functional object at 0x7f532b956550>,\n",
            "  0.0625),\n",
            " (<tensorflow.python.keras.engine.functional.Functional object at 0x7f532b205dc0>,\n",
            "  0.0625),\n",
            " (<tensorflow.python.keras.engine.functional.Functional object at 0x7f532be1aca0>,\n",
            "  0.0625)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfbBceob7QPw"
      },
      "source": [
        "# produce predictions on test set for submission\n",
        "\n",
        "# Modify paths as per your need\n",
        "\n",
        "test_path = \"./test-faces/\"\n",
        "submission = pd.read_csv('./test-full.csv')\n",
        "predictions = []\n",
        "\n",
        "for i in range(0, len(submission.p1_new.values), 32):\n",
        "    print(i)\n",
        "    X1 = submission.p1_new.values[i:i+32]\n",
        "    X2 = submission.p2_new.values[i:i+32]\n",
        "\n",
        "    input_1 = np.array([read_img(test_path + x) for x in X1])\n",
        "    input_2 = np.array([read_img(test_path + x) for x in X2])\n",
        "\n",
        "    input_1_2 = np.array([np.flip(x, axis=1) for x in input_1])\n",
        "    input_2_2 = np.array([np.flip(x, axis=1) for x in input_2])\n",
        "\n",
        "    input_1_3 = np.array([np.flip(x, axis=-1) for x in input_1])\n",
        "    input_2_3 = np.array([np.flip(x, axis=-1) for x in input_2])\n",
        "\n",
        "    preds = []\n",
        "    for (model, weight) in models: \n",
        "        #pred = model.predict([input_1, input_2], verbose=0) * weight\n",
        "        pred1 = model.predict([input_1, input_2], verbose=0) \n",
        "        pred2 = model.predict([input_1_2, input_2_2], verbose=0) \n",
        "        pred3 = model.predict([input_1_3, input_2_3], verbose=0)\n",
        "        pred = (pred1 + pred2 + pred3) * weight / 3\n",
        "        preds.append(pred)\n",
        "    preds = np.array(preds).sum(axis=0).squeeze().tolist()\n",
        "    \n",
        "    predictions.extend(preds)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH-q_0GjyzZB",
        "outputId": "9e34ae23-5b17-424f-c219-7a0fa6a6810f"
      },
      "source": [
        "d = {'index': np.arange(0, len(predictions), 1), 'label':predictions}\n",
        "submissionfile  = pd.DataFrame(data=d)\n",
        "print(submissionfile[\"label\"].min(), submissionfile[\"label\"].max(), submissionfile[\"label\"].mean(), submissionfile[\"label\"].median())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.00943293608725071 0.9998732209205627 0.49000495099264435 0.47689127922058105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XanTMS-AKDPj",
        "outputId": "973c2b5e-63c8-4f91-d43f-01e3004254e8"
      },
      "source": [
        "temp = (submissionfile[\"label\"] > submissionfile[\"label\"].median()).astype(int)\n",
        "#temp = (submissionfile[\"label\"] > 0.5).astype(int)\n",
        "print(\"acc\", ((temp[:19890]).sum() + len(submissionfile) -19889 - (temp[19890:]).sum())/len(submissionfile))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc 0.7667010542737086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngGUlTRSQjaN"
      },
      "source": [
        "submissionfile[\"label\"] = (submissionfile[\"label\"] > submissionfile[\"label\"].median()).astype(int)\n",
        "#submissionfile[\"label\"] = (submissionfile[\"label\"] > 0.5).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-qJxpm4QnCM"
      },
      "source": [
        "submissionfile.astype('int64').to_csv(\"./predictions.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR1313e_mr0l"
      },
      "source": [
        "# produce predictions on test set for submission\n",
        "\n",
        "# Modify paths as per your need\n",
        "\n",
        "test_path = \"./kinship_dataset/test-faces/\"\n",
        "submission = pd.read_csv('./kinship_dataset/test-full.csv')\n",
        "predictions_ = {}\n",
        "\n",
        "batch_size = 64\n",
        "for i in range(0, len(submission.p1_new.values), batch_size):\n",
        "    print(i)\n",
        "    X1 = submission.p1_new.values[i:i+batch_size]\n",
        "    X2 = submission.p2_new.values[i:i+batch_size]\n",
        "\n",
        "    input_1 = np.array([read_img(test_path + x) for x in X1])\n",
        "    input_2 = np.array([read_img(test_path + x) for x in X2])\n",
        "\n",
        "    input_1_2 = np.array([np.flip(x, axis=1) for x in input_1])\n",
        "    input_2_2 = np.array([np.flip(x, axis=1) for x in input_2])\n",
        "\n",
        "    input_1_3 = np.array([np.flip(x, axis=-1) for x in input_1])\n",
        "    input_2_3 = np.array([np.flip(x, axis=-1) for x in input_2])\n",
        "\n",
        "    preds = []\n",
        "    for (model, weight) in models: \n",
        "        pred1 = model([input_1, input_2], training=False) \n",
        "        pred2 = model([input_1_2, input_2_2], training=False) \n",
        "        pred3 = model([input_1_3, input_2_3], training=False) \n",
        "        pred = (pred1 + pred2 + pred3) * weight / 3\n",
        "        preds.append(pred)\n",
        "\n",
        "    preds = (np.array(preds).sum(axis=0).squeeze() > 0.47689127922058105).astype(int)\n",
        "    labels = submission.labels[i:i+batch_size]\n",
        "    right_or_wrong = (preds == np.array(labels)).tolist()\n",
        "\n",
        "    ptypes = submission.ptype[i:i+batch_size].tolist()\n",
        "    for j in range(len(ptypes)): \n",
        "        ptype = ptypes[j]\n",
        "        predictions_.setdefault(ptype, {\"count\": 0, \"correct\": 0})\n",
        "        predictions_[ptype][\"correct\"] += right_or_wrong[j]\n",
        "        predictions_[ptype][\"count\"] += 1\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhpD1hfF8CV2",
        "outputId": "32eb188c-1cee-4058-aa5d-fdffb9b8d68f"
      },
      "source": [
        "for ptype in predictions_: \n",
        "  print(ptype, predictions_[ptype][\"correct\"] * 1.0/ predictions_[ptype][\"count\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bb 0.8291413703382481\n",
            "ss 0.8301759133964818\n",
            "fd 0.7164624047697913\n",
            "md 0.7650753768844221\n",
            "fs 0.7613809960281087\n",
            "gmgd 0.29577464788732394\n",
            "gmgs 0.3333333333333333\n",
            "ms 0.7229323308270676\n",
            "gfgs 0.53125\n",
            "gfgd 0.4628099173553719\n",
            "sibs 0.7776628748707343\n",
            "nan 0.766886616632247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmjsaP9B6xZG",
        "outputId": "21f1deab-af57-42cc-e298-8d4dbd09b6d1"
      },
      "source": [
        "# ensemble of model_1\n",
        "\n",
        "test_path = \"./kinship_dataset/test-faces/\"\n",
        "submission = pd.read_csv('./kinship_dataset/test-full.csv')\n",
        "predictions_ = {}\n",
        "\n",
        "batch_size = 64\n",
        "for i in range(0, len(submission.p1_new.values), batch_size):\n",
        "    print(i)\n",
        "    X1 = submission.p1_new.values[i:i+batch_size]\n",
        "    X2 = submission.p2_new.values[i:i+batch_size]\n",
        "\n",
        "    input_1 = np.array([read_img(test_path + x) for x in X1])\n",
        "    input_2 = np.array([read_img(test_path + x) for x in X2])\n",
        "\n",
        "    input_1_2 = np.array([np.flip(x, axis=1) for x in input_1])\n",
        "    input_2_2 = np.array([np.flip(x, axis=1) for x in input_2])\n",
        "\n",
        "    input_1_3 = np.array([np.flip(x, axis=-1) for x in input_1])\n",
        "    input_2_3 = np.array([np.flip(x, axis=-1) for x in input_2])\n",
        "\n",
        "    preds = []\n",
        "    for (model, weight) in models[0:4]: \n",
        "        pred1 = model([input_1, input_2], training=False) \n",
        "        pred2 = model([input_1_2, input_2_2], training=False) \n",
        "        pred3 = model([input_1_3, input_2_3], training=False) \n",
        "        pred = (pred1 + pred2 + pred3) * weight * 4 / 3\n",
        "        preds.append(pred)\n",
        "\n",
        "    preds = (np.array(preds).sum(axis=0).squeeze() > 0.47689127922058105).astype(int)\n",
        "    labels = submission.labels[i:i+batch_size]\n",
        "    right_or_wrong = (preds == np.array(labels)).tolist()\n",
        "\n",
        "    ptypes = submission.ptype[i:i+batch_size].tolist()\n",
        "    for j in range(len(ptypes)): \n",
        "        ptype = ptypes[j]\n",
        "        predictions_.setdefault(ptype, {\"count\": 0, \"correct\": 0})\n",
        "        predictions_[ptype][\"correct\"] += right_or_wrong[j]\n",
        "        predictions_[ptype][\"count\"] += 1\n",
        "    \n",
        "for ptype in predictions_: \n",
        "    print(ptype, predictions_[ptype][\"correct\"] * 1.0/ predictions_[ptype][\"count\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "64\n",
            "128\n",
            "192\n",
            "256\n",
            "320\n",
            "384\n",
            "448\n",
            "512\n",
            "576\n",
            "640\n",
            "704\n",
            "768\n",
            "832\n",
            "896\n",
            "960\n",
            "1024\n",
            "1088\n",
            "1152\n",
            "1216\n",
            "1280\n",
            "1344\n",
            "1408\n",
            "1472\n",
            "1536\n",
            "1600\n",
            "1664\n",
            "1728\n",
            "1792\n",
            "1856\n",
            "1920\n",
            "1984\n",
            "2048\n",
            "2112\n",
            "2176\n",
            "2240\n",
            "2304\n",
            "2368\n",
            "2432\n",
            "2496\n",
            "2560\n",
            "2624\n",
            "2688\n",
            "2752\n",
            "2816\n",
            "2880\n",
            "2944\n",
            "3008\n",
            "3072\n",
            "3136\n",
            "3200\n",
            "3264\n",
            "3328\n",
            "3392\n",
            "3456\n",
            "3520\n",
            "3584\n",
            "3648\n",
            "3712\n",
            "3776\n",
            "3840\n",
            "3904\n",
            "3968\n",
            "4032\n",
            "4096\n",
            "4160\n",
            "4224\n",
            "4288\n",
            "4352\n",
            "4416\n",
            "4480\n",
            "4544\n",
            "4608\n",
            "4672\n",
            "4736\n",
            "4800\n",
            "4864\n",
            "4928\n",
            "4992\n",
            "5056\n",
            "5120\n",
            "5184\n",
            "5248\n",
            "5312\n",
            "5376\n",
            "5440\n",
            "5504\n",
            "5568\n",
            "5632\n",
            "5696\n",
            "5760\n",
            "5824\n",
            "5888\n",
            "5952\n",
            "6016\n",
            "6080\n",
            "6144\n",
            "6208\n",
            "6272\n",
            "6336\n",
            "6400\n",
            "6464\n",
            "6528\n",
            "6592\n",
            "6656\n",
            "6720\n",
            "6784\n",
            "6848\n",
            "6912\n",
            "6976\n",
            "7040\n",
            "7104\n",
            "7168\n",
            "7232\n",
            "7296\n",
            "7360\n",
            "7424\n",
            "7488\n",
            "7552\n",
            "7616\n",
            "7680\n",
            "7744\n",
            "7808\n",
            "7872\n",
            "7936\n",
            "8000\n",
            "8064\n",
            "8128\n",
            "8192\n",
            "8256\n",
            "8320\n",
            "8384\n",
            "8448\n",
            "8512\n",
            "8576\n",
            "8640\n",
            "8704\n",
            "8768\n",
            "8832\n",
            "8896\n",
            "8960\n",
            "9024\n",
            "9088\n",
            "9152\n",
            "9216\n",
            "9280\n",
            "9344\n",
            "9408\n",
            "9472\n",
            "9536\n",
            "9600\n",
            "9664\n",
            "9728\n",
            "9792\n",
            "9856\n",
            "9920\n",
            "9984\n",
            "10048\n",
            "10112\n",
            "10176\n",
            "10240\n",
            "10304\n",
            "10368\n",
            "10432\n",
            "10496\n",
            "10560\n",
            "10624\n",
            "10688\n",
            "10752\n",
            "10816\n",
            "10880\n",
            "10944\n",
            "11008\n",
            "11072\n",
            "11136\n",
            "11200\n",
            "11264\n",
            "11328\n",
            "11392\n",
            "11456\n",
            "11520\n",
            "11584\n",
            "11648\n",
            "11712\n",
            "11776\n",
            "11840\n",
            "11904\n",
            "11968\n",
            "12032\n",
            "12096\n",
            "12160\n",
            "12224\n",
            "12288\n",
            "12352\n",
            "12416\n",
            "12480\n",
            "12544\n",
            "12608\n",
            "12672\n",
            "12736\n",
            "12800\n",
            "12864\n",
            "12928\n",
            "12992\n",
            "13056\n",
            "13120\n",
            "13184\n",
            "13248\n",
            "13312\n",
            "13376\n",
            "13440\n",
            "13504\n",
            "13568\n",
            "13632\n",
            "13696\n",
            "13760\n",
            "13824\n",
            "13888\n",
            "13952\n",
            "14016\n",
            "14080\n",
            "14144\n",
            "14208\n",
            "14272\n",
            "14336\n",
            "14400\n",
            "14464\n",
            "14528\n",
            "14592\n",
            "14656\n",
            "14720\n",
            "14784\n",
            "14848\n",
            "14912\n",
            "14976\n",
            "15040\n",
            "15104\n",
            "15168\n",
            "15232\n",
            "15296\n",
            "15360\n",
            "15424\n",
            "15488\n",
            "15552\n",
            "15616\n",
            "15680\n",
            "15744\n",
            "15808\n",
            "15872\n",
            "15936\n",
            "16000\n",
            "16064\n",
            "16128\n",
            "16192\n",
            "16256\n",
            "16320\n",
            "16384\n",
            "16448\n",
            "16512\n",
            "16576\n",
            "16640\n",
            "16704\n",
            "16768\n",
            "16832\n",
            "16896\n",
            "16960\n",
            "17024\n",
            "17088\n",
            "17152\n",
            "17216\n",
            "17280\n",
            "17344\n",
            "17408\n",
            "17472\n",
            "17536\n",
            "17600\n",
            "17664\n",
            "17728\n",
            "17792\n",
            "17856\n",
            "17920\n",
            "17984\n",
            "18048\n",
            "18112\n",
            "18176\n",
            "18240\n",
            "18304\n",
            "18368\n",
            "18432\n",
            "18496\n",
            "18560\n",
            "18624\n",
            "18688\n",
            "18752\n",
            "18816\n",
            "18880\n",
            "18944\n",
            "19008\n",
            "19072\n",
            "19136\n",
            "19200\n",
            "19264\n",
            "19328\n",
            "19392\n",
            "19456\n",
            "19520\n",
            "19584\n",
            "19648\n",
            "19712\n",
            "19776\n",
            "19840\n",
            "19904\n",
            "19968\n",
            "20032\n",
            "20096\n",
            "20160\n",
            "20224\n",
            "20288\n",
            "20352\n",
            "20416\n",
            "20480\n",
            "20544\n",
            "20608\n",
            "20672\n",
            "20736\n",
            "20800\n",
            "20864\n",
            "20928\n",
            "20992\n",
            "21056\n",
            "21120\n",
            "21184\n",
            "21248\n",
            "21312\n",
            "21376\n",
            "21440\n",
            "21504\n",
            "21568\n",
            "21632\n",
            "21696\n",
            "21760\n",
            "21824\n",
            "21888\n",
            "21952\n",
            "22016\n",
            "22080\n",
            "22144\n",
            "22208\n",
            "22272\n",
            "22336\n",
            "22400\n",
            "22464\n",
            "22528\n",
            "22592\n",
            "22656\n",
            "22720\n",
            "22784\n",
            "22848\n",
            "22912\n",
            "22976\n",
            "23040\n",
            "23104\n",
            "23168\n",
            "23232\n",
            "23296\n",
            "23360\n",
            "23424\n",
            "23488\n",
            "23552\n",
            "23616\n",
            "23680\n",
            "23744\n",
            "23808\n",
            "23872\n",
            "23936\n",
            "24000\n",
            "24064\n",
            "24128\n",
            "24192\n",
            "24256\n",
            "24320\n",
            "24384\n",
            "24448\n",
            "24512\n",
            "24576\n",
            "24640\n",
            "24704\n",
            "24768\n",
            "24832\n",
            "24896\n",
            "24960\n",
            "25024\n",
            "25088\n",
            "25152\n",
            "25216\n",
            "25280\n",
            "25344\n",
            "25408\n",
            "25472\n",
            "25536\n",
            "25600\n",
            "25664\n",
            "25728\n",
            "25792\n",
            "25856\n",
            "25920\n",
            "25984\n",
            "26048\n",
            "26112\n",
            "26176\n",
            "26240\n",
            "26304\n",
            "26368\n",
            "26432\n",
            "26496\n",
            "26560\n",
            "26624\n",
            "26688\n",
            "26752\n",
            "26816\n",
            "26880\n",
            "26944\n",
            "27008\n",
            "27072\n",
            "27136\n",
            "27200\n",
            "27264\n",
            "27328\n",
            "27392\n",
            "27456\n",
            "27520\n",
            "27584\n",
            "27648\n",
            "27712\n",
            "27776\n",
            "27840\n",
            "27904\n",
            "27968\n",
            "28032\n",
            "28096\n",
            "28160\n",
            "28224\n",
            "28288\n",
            "28352\n",
            "28416\n",
            "28480\n",
            "28544\n",
            "28608\n",
            "28672\n",
            "28736\n",
            "28800\n",
            "28864\n",
            "28928\n",
            "28992\n",
            "29056\n",
            "29120\n",
            "29184\n",
            "29248\n",
            "29312\n",
            "29376\n",
            "29440\n",
            "29504\n",
            "29568\n",
            "29632\n",
            "29696\n",
            "29760\n",
            "29824\n",
            "29888\n",
            "29952\n",
            "30016\n",
            "30080\n",
            "30144\n",
            "30208\n",
            "30272\n",
            "30336\n",
            "30400\n",
            "30464\n",
            "30528\n",
            "30592\n",
            "30656\n",
            "30720\n",
            "30784\n",
            "30848\n",
            "30912\n",
            "30976\n",
            "31040\n",
            "31104\n",
            "31168\n",
            "31232\n",
            "31296\n",
            "31360\n",
            "31424\n",
            "31488\n",
            "31552\n",
            "31616\n",
            "31680\n",
            "31744\n",
            "31808\n",
            "31872\n",
            "31936\n",
            "32000\n",
            "32064\n",
            "32128\n",
            "32192\n",
            "32256\n",
            "32320\n",
            "32384\n",
            "32448\n",
            "32512\n",
            "32576\n",
            "32640\n",
            "32704\n",
            "32768\n",
            "32832\n",
            "32896\n",
            "32960\n",
            "33024\n",
            "33088\n",
            "33152\n",
            "33216\n",
            "33280\n",
            "33344\n",
            "33408\n",
            "33472\n",
            "33536\n",
            "33600\n",
            "33664\n",
            "33728\n",
            "33792\n",
            "33856\n",
            "33920\n",
            "33984\n",
            "34048\n",
            "34112\n",
            "34176\n",
            "34240\n",
            "34304\n",
            "34368\n",
            "34432\n",
            "34496\n",
            "34560\n",
            "34624\n",
            "34688\n",
            "34752\n",
            "34816\n",
            "34880\n",
            "34944\n",
            "35008\n",
            "35072\n",
            "35136\n",
            "35200\n",
            "35264\n",
            "35328\n",
            "35392\n",
            "35456\n",
            "35520\n",
            "35584\n",
            "35648\n",
            "35712\n",
            "35776\n",
            "35840\n",
            "35904\n",
            "35968\n",
            "36032\n",
            "36096\n",
            "36160\n",
            "36224\n",
            "36288\n",
            "36352\n",
            "36416\n",
            "36480\n",
            "36544\n",
            "36608\n",
            "36672\n",
            "36736\n",
            "36800\n",
            "36864\n",
            "36928\n",
            "36992\n",
            "37056\n",
            "37120\n",
            "37184\n",
            "37248\n",
            "37312\n",
            "37376\n",
            "37440\n",
            "37504\n",
            "37568\n",
            "37632\n",
            "37696\n",
            "37760\n",
            "37824\n",
            "37888\n",
            "37952\n",
            "38016\n",
            "38080\n",
            "38144\n",
            "38208\n",
            "38272\n",
            "38336\n",
            "38400\n",
            "38464\n",
            "38528\n",
            "38592\n",
            "38656\n",
            "38720\n",
            "38784\n",
            "38848\n",
            "38912\n",
            "38976\n",
            "39040\n",
            "39104\n",
            "39168\n",
            "39232\n",
            "39296\n",
            "39360\n",
            "39424\n",
            "39488\n",
            "39552\n",
            "39616\n",
            "39680\n",
            "bb 0.821335646140503\n",
            "ss 0.8203653585926928\n",
            "fd 0.7257369990062935\n",
            "md 0.772927135678392\n",
            "fs 0.7641307668805377\n",
            "gmgd 0.39436619718309857\n",
            "gmgs 0.39285714285714285\n",
            "ms 0.7345864661654136\n",
            "gfgs 0.4895833333333333\n",
            "gfgd 0.5371900826446281\n",
            "sibs 0.7456049638055843\n",
            "nan 0.7322319044980607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAgE76Y80PHm",
        "outputId": "bbe5c3c6-2a87-4025-a546-6b463c7d29d3"
      },
      "source": [
        "correct_sum = 0\n",
        "count_sum = 0\n",
        "for ptype in predictions_: \n",
        "    correct_sum += predictions_[ptype][\"correct\"] \n",
        "    count_sum += predictions_[ptype][\"count\"]\n",
        "print(correct_sum * 1.0 / count_sum)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.74991822459301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLdN2F2P0PHn",
        "outputId": "64bdb1bd-a2f0-4380-ead2-977d799d7a2f"
      },
      "source": [
        "# ensemble of model_2\n",
        "\n",
        "test_path = \"./kinship_dataset/test-faces/\"\n",
        "submission = pd.read_csv('./kinship_dataset/test-full.csv')\n",
        "predictions_ = {}\n",
        "\n",
        "batch_size = 64\n",
        "for i in range(0, len(submission.p1_new.values), batch_size):\n",
        "    print(i)\n",
        "    X1 = submission.p1_new.values[i:i+batch_size]\n",
        "    X2 = submission.p2_new.values[i:i+batch_size]\n",
        "\n",
        "    input_1 = np.array([read_img(test_path + x) for x in X1])\n",
        "    input_2 = np.array([read_img(test_path + x) for x in X2])\n",
        "\n",
        "    input_1_2 = np.array([np.flip(x, axis=1) for x in input_1])\n",
        "    input_2_2 = np.array([np.flip(x, axis=1) for x in input_2])\n",
        "\n",
        "    input_1_3 = np.array([np.flip(x, axis=-1) for x in input_1])\n",
        "    input_2_3 = np.array([np.flip(x, axis=-1) for x in input_2])\n",
        "\n",
        "    preds = []\n",
        "    for (model, weight) in models[4:8]: \n",
        "        pred1 = model([input_1, input_2], training=False) \n",
        "        pred2 = model([input_1_2, input_2_2], training=False) \n",
        "        pred3 = model([input_1_3, input_2_3], training=False) \n",
        "        pred = (pred1 + pred2 + pred3) * weight * 4 / 3\n",
        "        preds.append(pred)\n",
        "\n",
        "    preds = (np.array(preds).sum(axis=0).squeeze() > 0.47689127922058105).astype(int)\n",
        "    labels = submission.labels[i:i+batch_size]\n",
        "    right_or_wrong = (preds == np.array(labels)).tolist()\n",
        "\n",
        "    ptypes = submission.ptype[i:i+batch_size].tolist()\n",
        "    for j in range(len(ptypes)): \n",
        "        ptype = ptypes[j]\n",
        "        predictions_.setdefault(ptype, {\"count\": 0, \"correct\": 0})\n",
        "        predictions_[ptype][\"correct\"] += right_or_wrong[j]\n",
        "        predictions_[ptype][\"count\"] += 1\n",
        "    \n",
        "correct_sum = 0\n",
        "count_sum = 0    \n",
        "for ptype in predictions_: \n",
        "    print(ptype, predictions_[ptype][\"correct\"] * 1.0/ predictions_[ptype][\"count\"])\n",
        "    correct_sum += predictions_[ptype][\"correct\"] \n",
        "    count_sum += predictions_[ptype][\"count\"]\n",
        "print(correct_sum * 1.0 / count_sum)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "64\n",
            "128\n",
            "192\n",
            "256\n",
            "320\n",
            "384\n",
            "448\n",
            "512\n",
            "576\n",
            "640\n",
            "704\n",
            "768\n",
            "832\n",
            "896\n",
            "960\n",
            "1024\n",
            "1088\n",
            "1152\n",
            "1216\n",
            "1280\n",
            "1344\n",
            "1408\n",
            "1472\n",
            "1536\n",
            "1600\n",
            "1664\n",
            "1728\n",
            "1792\n",
            "1856\n",
            "1920\n",
            "1984\n",
            "2048\n",
            "2112\n",
            "2176\n",
            "2240\n",
            "2304\n",
            "2368\n",
            "2432\n",
            "2496\n",
            "2560\n",
            "2624\n",
            "2688\n",
            "2752\n",
            "2816\n",
            "2880\n",
            "2944\n",
            "3008\n",
            "3072\n",
            "3136\n",
            "3200\n",
            "3264\n",
            "3328\n",
            "3392\n",
            "3456\n",
            "3520\n",
            "3584\n",
            "3648\n",
            "3712\n",
            "3776\n",
            "3840\n",
            "3904\n",
            "3968\n",
            "4032\n",
            "4096\n",
            "4160\n",
            "4224\n",
            "4288\n",
            "4352\n",
            "4416\n",
            "4480\n",
            "4544\n",
            "4608\n",
            "4672\n",
            "4736\n",
            "4800\n",
            "4864\n",
            "4928\n",
            "4992\n",
            "5056\n",
            "5120\n",
            "5184\n",
            "5248\n",
            "5312\n",
            "5376\n",
            "5440\n",
            "5504\n",
            "5568\n",
            "5632\n",
            "5696\n",
            "5760\n",
            "5824\n",
            "5888\n",
            "5952\n",
            "6016\n",
            "6080\n",
            "6144\n",
            "6208\n",
            "6272\n",
            "6336\n",
            "6400\n",
            "6464\n",
            "6528\n",
            "6592\n",
            "6656\n",
            "6720\n",
            "6784\n",
            "6848\n",
            "6912\n",
            "6976\n",
            "7040\n",
            "7104\n",
            "7168\n",
            "7232\n",
            "7296\n",
            "7360\n",
            "7424\n",
            "7488\n",
            "7552\n",
            "7616\n",
            "7680\n",
            "7744\n",
            "7808\n",
            "7872\n",
            "7936\n",
            "8000\n",
            "8064\n",
            "8128\n",
            "8192\n",
            "8256\n",
            "8320\n",
            "8384\n",
            "8448\n",
            "8512\n",
            "8576\n",
            "8640\n",
            "8704\n",
            "8768\n",
            "8832\n",
            "8896\n",
            "8960\n",
            "9024\n",
            "9088\n",
            "9152\n",
            "9216\n",
            "9280\n",
            "9344\n",
            "9408\n",
            "9472\n",
            "9536\n",
            "9600\n",
            "9664\n",
            "9728\n",
            "9792\n",
            "9856\n",
            "9920\n",
            "9984\n",
            "10048\n",
            "10112\n",
            "10176\n",
            "10240\n",
            "10304\n",
            "10368\n",
            "10432\n",
            "10496\n",
            "10560\n",
            "10624\n",
            "10688\n",
            "10752\n",
            "10816\n",
            "10880\n",
            "10944\n",
            "11008\n",
            "11072\n",
            "11136\n",
            "11200\n",
            "11264\n",
            "11328\n",
            "11392\n",
            "11456\n",
            "11520\n",
            "11584\n",
            "11648\n",
            "11712\n",
            "11776\n",
            "11840\n",
            "11904\n",
            "11968\n",
            "12032\n",
            "12096\n",
            "12160\n",
            "12224\n",
            "12288\n",
            "12352\n",
            "12416\n",
            "12480\n",
            "12544\n",
            "12608\n",
            "12672\n",
            "12736\n",
            "12800\n",
            "12864\n",
            "12928\n",
            "12992\n",
            "13056\n",
            "13120\n",
            "13184\n",
            "13248\n",
            "13312\n",
            "13376\n",
            "13440\n",
            "13504\n",
            "13568\n",
            "13632\n",
            "13696\n",
            "13760\n",
            "13824\n",
            "13888\n",
            "13952\n",
            "14016\n",
            "14080\n",
            "14144\n",
            "14208\n",
            "14272\n",
            "14336\n",
            "14400\n",
            "14464\n",
            "14528\n",
            "14592\n",
            "14656\n",
            "14720\n",
            "14784\n",
            "14848\n",
            "14912\n",
            "14976\n",
            "15040\n",
            "15104\n",
            "15168\n",
            "15232\n",
            "15296\n",
            "15360\n",
            "15424\n",
            "15488\n",
            "15552\n",
            "15616\n",
            "15680\n",
            "15744\n",
            "15808\n",
            "15872\n",
            "15936\n",
            "16000\n",
            "16064\n",
            "16128\n",
            "16192\n",
            "16256\n",
            "16320\n",
            "16384\n",
            "16448\n",
            "16512\n",
            "16576\n",
            "16640\n",
            "16704\n",
            "16768\n",
            "16832\n",
            "16896\n",
            "16960\n",
            "17024\n",
            "17088\n",
            "17152\n",
            "17216\n",
            "17280\n",
            "17344\n",
            "17408\n",
            "17472\n",
            "17536\n",
            "17600\n",
            "17664\n",
            "17728\n",
            "17792\n",
            "17856\n",
            "17920\n",
            "17984\n",
            "18048\n",
            "18112\n",
            "18176\n",
            "18240\n",
            "18304\n",
            "18368\n",
            "18432\n",
            "18496\n",
            "18560\n",
            "18624\n",
            "18688\n",
            "18752\n",
            "18816\n",
            "18880\n",
            "18944\n",
            "19008\n",
            "19072\n",
            "19136\n",
            "19200\n",
            "19264\n",
            "19328\n",
            "19392\n",
            "19456\n",
            "19520\n",
            "19584\n",
            "19648\n",
            "19712\n",
            "19776\n",
            "19840\n",
            "19904\n",
            "19968\n",
            "20032\n",
            "20096\n",
            "20160\n",
            "20224\n",
            "20288\n",
            "20352\n",
            "20416\n",
            "20480\n",
            "20544\n",
            "20608\n",
            "20672\n",
            "20736\n",
            "20800\n",
            "20864\n",
            "20928\n",
            "20992\n",
            "21056\n",
            "21120\n",
            "21184\n",
            "21248\n",
            "21312\n",
            "21376\n",
            "21440\n",
            "21504\n",
            "21568\n",
            "21632\n",
            "21696\n",
            "21760\n",
            "21824\n",
            "21888\n",
            "21952\n",
            "22016\n",
            "22080\n",
            "22144\n",
            "22208\n",
            "22272\n",
            "22336\n",
            "22400\n",
            "22464\n",
            "22528\n",
            "22592\n",
            "22656\n",
            "22720\n",
            "22784\n",
            "22848\n",
            "22912\n",
            "22976\n",
            "23040\n",
            "23104\n",
            "23168\n",
            "23232\n",
            "23296\n",
            "23360\n",
            "23424\n",
            "23488\n",
            "23552\n",
            "23616\n",
            "23680\n",
            "23744\n",
            "23808\n",
            "23872\n",
            "23936\n",
            "24000\n",
            "24064\n",
            "24128\n",
            "24192\n",
            "24256\n",
            "24320\n",
            "24384\n",
            "24448\n",
            "24512\n",
            "24576\n",
            "24640\n",
            "24704\n",
            "24768\n",
            "24832\n",
            "24896\n",
            "24960\n",
            "25024\n",
            "25088\n",
            "25152\n",
            "25216\n",
            "25280\n",
            "25344\n",
            "25408\n",
            "25472\n",
            "25536\n",
            "25600\n",
            "25664\n",
            "25728\n",
            "25792\n",
            "25856\n",
            "25920\n",
            "25984\n",
            "26048\n",
            "26112\n",
            "26176\n",
            "26240\n",
            "26304\n",
            "26368\n",
            "26432\n",
            "26496\n",
            "26560\n",
            "26624\n",
            "26688\n",
            "26752\n",
            "26816\n",
            "26880\n",
            "26944\n",
            "27008\n",
            "27072\n",
            "27136\n",
            "27200\n",
            "27264\n",
            "27328\n",
            "27392\n",
            "27456\n",
            "27520\n",
            "27584\n",
            "27648\n",
            "27712\n",
            "27776\n",
            "27840\n",
            "27904\n",
            "27968\n",
            "28032\n",
            "28096\n",
            "28160\n",
            "28224\n",
            "28288\n",
            "28352\n",
            "28416\n",
            "28480\n",
            "28544\n",
            "28608\n",
            "28672\n",
            "28736\n",
            "28800\n",
            "28864\n",
            "28928\n",
            "28992\n",
            "29056\n",
            "29120\n",
            "29184\n",
            "29248\n",
            "29312\n",
            "29376\n",
            "29440\n",
            "29504\n",
            "29568\n",
            "29632\n",
            "29696\n",
            "29760\n",
            "29824\n",
            "29888\n",
            "29952\n",
            "30016\n",
            "30080\n",
            "30144\n",
            "30208\n",
            "30272\n",
            "30336\n",
            "30400\n",
            "30464\n",
            "30528\n",
            "30592\n",
            "30656\n",
            "30720\n",
            "30784\n",
            "30848\n",
            "30912\n",
            "30976\n",
            "31040\n",
            "31104\n",
            "31168\n",
            "31232\n",
            "31296\n",
            "31360\n",
            "31424\n",
            "31488\n",
            "31552\n",
            "31616\n",
            "31680\n",
            "31744\n",
            "31808\n",
            "31872\n",
            "31936\n",
            "32000\n",
            "32064\n",
            "32128\n",
            "32192\n",
            "32256\n",
            "32320\n",
            "32384\n",
            "32448\n",
            "32512\n",
            "32576\n",
            "32640\n",
            "32704\n",
            "32768\n",
            "32832\n",
            "32896\n",
            "32960\n",
            "33024\n",
            "33088\n",
            "33152\n",
            "33216\n",
            "33280\n",
            "33344\n",
            "33408\n",
            "33472\n",
            "33536\n",
            "33600\n",
            "33664\n",
            "33728\n",
            "33792\n",
            "33856\n",
            "33920\n",
            "33984\n",
            "34048\n",
            "34112\n",
            "34176\n",
            "34240\n",
            "34304\n",
            "34368\n",
            "34432\n",
            "34496\n",
            "34560\n",
            "34624\n",
            "34688\n",
            "34752\n",
            "34816\n",
            "34880\n",
            "34944\n",
            "35008\n",
            "35072\n",
            "35136\n",
            "35200\n",
            "35264\n",
            "35328\n",
            "35392\n",
            "35456\n",
            "35520\n",
            "35584\n",
            "35648\n",
            "35712\n",
            "35776\n",
            "35840\n",
            "35904\n",
            "35968\n",
            "36032\n",
            "36096\n",
            "36160\n",
            "36224\n",
            "36288\n",
            "36352\n",
            "36416\n",
            "36480\n",
            "36544\n",
            "36608\n",
            "36672\n",
            "36736\n",
            "36800\n",
            "36864\n",
            "36928\n",
            "36992\n",
            "37056\n",
            "37120\n",
            "37184\n",
            "37248\n",
            "37312\n",
            "37376\n",
            "37440\n",
            "37504\n",
            "37568\n",
            "37632\n",
            "37696\n",
            "37760\n",
            "37824\n",
            "37888\n",
            "37952\n",
            "38016\n",
            "38080\n",
            "38144\n",
            "38208\n",
            "38272\n",
            "38336\n",
            "38400\n",
            "38464\n",
            "38528\n",
            "38592\n",
            "38656\n",
            "38720\n",
            "38784\n",
            "38848\n",
            "38912\n",
            "38976\n",
            "39040\n",
            "39104\n",
            "39168\n",
            "39232\n",
            "39296\n",
            "39360\n",
            "39424\n",
            "39488\n",
            "39552\n",
            "39616\n",
            "39680\n",
            "bb 0.8256721595836947\n",
            "ss 0.8152909336941814\n",
            "fd 0.7386551838357072\n",
            "md 0.7512562814070352\n",
            "fs 0.7778796211426826\n",
            "gmgd 0.323943661971831\n",
            "gmgs 0.34523809523809523\n",
            "ms 0.7357142857142858\n",
            "gfgs 0.5208333333333334\n",
            "gfgd 0.4380165289256198\n",
            "sibs 0.7693898655635988\n",
            "nan 0.7544451720143052\n",
            "0.7615932365448004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg3MBw3Q0PHn",
        "outputId": "c367d0f5-5cc4-442c-8d1d-b10dfeff5824"
      },
      "source": [
        "# ensemble of model_3\n",
        "\n",
        "test_path = \"./kinship_dataset/test-faces/\"\n",
        "submission = pd.read_csv('./kinship_dataset/test-full.csv')\n",
        "predictions_ = {}\n",
        "\n",
        "batch_size = 64\n",
        "for i in range(0, len(submission.p1_new.values), batch_size):\n",
        "    print(i)\n",
        "    X1 = submission.p1_new.values[i:i+batch_size]\n",
        "    X2 = submission.p2_new.values[i:i+batch_size]\n",
        "\n",
        "    input_1 = np.array([read_img(test_path + x) for x in X1])\n",
        "    input_2 = np.array([read_img(test_path + x) for x in X2])\n",
        "\n",
        "    input_1_2 = np.array([np.flip(x, axis=1) for x in input_1])\n",
        "    input_2_2 = np.array([np.flip(x, axis=1) for x in input_2])\n",
        "\n",
        "    input_1_3 = np.array([np.flip(x, axis=-1) for x in input_1])\n",
        "    input_2_3 = np.array([np.flip(x, axis=-1) for x in input_2])\n",
        "\n",
        "    preds = []\n",
        "    for (model, weight) in models[8:12]: \n",
        "        pred1 = model([input_1, input_2], training=False) \n",
        "        pred2 = model([input_1_2, input_2_2], training=False) \n",
        "        pred3 = model([input_1_3, input_2_3], training=False) \n",
        "        pred = (pred1 + pred2 + pred3) * weight * 4 / 3\n",
        "        preds.append(pred)\n",
        "\n",
        "    preds = (np.array(preds).sum(axis=0).squeeze() > 0.47689127922058105).astype(int)\n",
        "    labels = submission.labels[i:i+batch_size]\n",
        "    right_or_wrong = (preds == np.array(labels)).tolist()\n",
        "\n",
        "    ptypes = submission.ptype[i:i+batch_size].tolist()\n",
        "    for j in range(len(ptypes)): \n",
        "        ptype = ptypes[j]\n",
        "        predictions_.setdefault(ptype, {\"count\": 0, \"correct\": 0})\n",
        "        predictions_[ptype][\"correct\"] += right_or_wrong[j]\n",
        "        predictions_[ptype][\"count\"] += 1\n",
        "    \n",
        "correct_sum = 0\n",
        "count_sum = 0    \n",
        "for ptype in predictions_: \n",
        "    print(ptype, predictions_[ptype][\"correct\"] * 1.0/ predictions_[ptype][\"count\"])\n",
        "    correct_sum += predictions_[ptype][\"correct\"] \n",
        "    count_sum += predictions_[ptype][\"count\"]\n",
        "print(correct_sum * 1.0 / count_sum)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "64\n",
            "128\n",
            "192\n",
            "256\n",
            "320\n",
            "384\n",
            "448\n",
            "512\n",
            "576\n",
            "640\n",
            "704\n",
            "768\n",
            "832\n",
            "896\n",
            "960\n",
            "1024\n",
            "1088\n",
            "1152\n",
            "1216\n",
            "1280\n",
            "1344\n",
            "1408\n",
            "1472\n",
            "1536\n",
            "1600\n",
            "1664\n",
            "1728\n",
            "1792\n",
            "1856\n",
            "1920\n",
            "1984\n",
            "2048\n",
            "2112\n",
            "2176\n",
            "2240\n",
            "2304\n",
            "2368\n",
            "2432\n",
            "2496\n",
            "2560\n",
            "2624\n",
            "2688\n",
            "2752\n",
            "2816\n",
            "2880\n",
            "2944\n",
            "3008\n",
            "3072\n",
            "3136\n",
            "3200\n",
            "3264\n",
            "3328\n",
            "3392\n",
            "3456\n",
            "3520\n",
            "3584\n",
            "3648\n",
            "3712\n",
            "3776\n",
            "3840\n",
            "3904\n",
            "3968\n",
            "4032\n",
            "4096\n",
            "4160\n",
            "4224\n",
            "4288\n",
            "4352\n",
            "4416\n",
            "4480\n",
            "4544\n",
            "4608\n",
            "4672\n",
            "4736\n",
            "4800\n",
            "4864\n",
            "4928\n",
            "4992\n",
            "5056\n",
            "5120\n",
            "5184\n",
            "5248\n",
            "5312\n",
            "5376\n",
            "5440\n",
            "5504\n",
            "5568\n",
            "5632\n",
            "5696\n",
            "5760\n",
            "5824\n",
            "5888\n",
            "5952\n",
            "6016\n",
            "6080\n",
            "6144\n",
            "6208\n",
            "6272\n",
            "6336\n",
            "6400\n",
            "6464\n",
            "6528\n",
            "6592\n",
            "6656\n",
            "6720\n",
            "6784\n",
            "6848\n",
            "6912\n",
            "6976\n",
            "7040\n",
            "7104\n",
            "7168\n",
            "7232\n",
            "7296\n",
            "7360\n",
            "7424\n",
            "7488\n",
            "7552\n",
            "7616\n",
            "7680\n",
            "7744\n",
            "7808\n",
            "7872\n",
            "7936\n",
            "8000\n",
            "8064\n",
            "8128\n",
            "8192\n",
            "8256\n",
            "8320\n",
            "8384\n",
            "8448\n",
            "8512\n",
            "8576\n",
            "8640\n",
            "8704\n",
            "8768\n",
            "8832\n",
            "8896\n",
            "8960\n",
            "9024\n",
            "9088\n",
            "9152\n",
            "9216\n",
            "9280\n",
            "9344\n",
            "9408\n",
            "9472\n",
            "9536\n",
            "9600\n",
            "9664\n",
            "9728\n",
            "9792\n",
            "9856\n",
            "9920\n",
            "9984\n",
            "10048\n",
            "10112\n",
            "10176\n",
            "10240\n",
            "10304\n",
            "10368\n",
            "10432\n",
            "10496\n",
            "10560\n",
            "10624\n",
            "10688\n",
            "10752\n",
            "10816\n",
            "10880\n",
            "10944\n",
            "11008\n",
            "11072\n",
            "11136\n",
            "11200\n",
            "11264\n",
            "11328\n",
            "11392\n",
            "11456\n",
            "11520\n",
            "11584\n",
            "11648\n",
            "11712\n",
            "11776\n",
            "11840\n",
            "11904\n",
            "11968\n",
            "12032\n",
            "12096\n",
            "12160\n",
            "12224\n",
            "12288\n",
            "12352\n",
            "12416\n",
            "12480\n",
            "12544\n",
            "12608\n",
            "12672\n",
            "12736\n",
            "12800\n",
            "12864\n",
            "12928\n",
            "12992\n",
            "13056\n",
            "13120\n",
            "13184\n",
            "13248\n",
            "13312\n",
            "13376\n",
            "13440\n",
            "13504\n",
            "13568\n",
            "13632\n",
            "13696\n",
            "13760\n",
            "13824\n",
            "13888\n",
            "13952\n",
            "14016\n",
            "14080\n",
            "14144\n",
            "14208\n",
            "14272\n",
            "14336\n",
            "14400\n",
            "14464\n",
            "14528\n",
            "14592\n",
            "14656\n",
            "14720\n",
            "14784\n",
            "14848\n",
            "14912\n",
            "14976\n",
            "15040\n",
            "15104\n",
            "15168\n",
            "15232\n",
            "15296\n",
            "15360\n",
            "15424\n",
            "15488\n",
            "15552\n",
            "15616\n",
            "15680\n",
            "15744\n",
            "15808\n",
            "15872\n",
            "15936\n",
            "16000\n",
            "16064\n",
            "16128\n",
            "16192\n",
            "16256\n",
            "16320\n",
            "16384\n",
            "16448\n",
            "16512\n",
            "16576\n",
            "16640\n",
            "16704\n",
            "16768\n",
            "16832\n",
            "16896\n",
            "16960\n",
            "17024\n",
            "17088\n",
            "17152\n",
            "17216\n",
            "17280\n",
            "17344\n",
            "17408\n",
            "17472\n",
            "17536\n",
            "17600\n",
            "17664\n",
            "17728\n",
            "17792\n",
            "17856\n",
            "17920\n",
            "17984\n",
            "18048\n",
            "18112\n",
            "18176\n",
            "18240\n",
            "18304\n",
            "18368\n",
            "18432\n",
            "18496\n",
            "18560\n",
            "18624\n",
            "18688\n",
            "18752\n",
            "18816\n",
            "18880\n",
            "18944\n",
            "19008\n",
            "19072\n",
            "19136\n",
            "19200\n",
            "19264\n",
            "19328\n",
            "19392\n",
            "19456\n",
            "19520\n",
            "19584\n",
            "19648\n",
            "19712\n",
            "19776\n",
            "19840\n",
            "19904\n",
            "19968\n",
            "20032\n",
            "20096\n",
            "20160\n",
            "20224\n",
            "20288\n",
            "20352\n",
            "20416\n",
            "20480\n",
            "20544\n",
            "20608\n",
            "20672\n",
            "20736\n",
            "20800\n",
            "20864\n",
            "20928\n",
            "20992\n",
            "21056\n",
            "21120\n",
            "21184\n",
            "21248\n",
            "21312\n",
            "21376\n",
            "21440\n",
            "21504\n",
            "21568\n",
            "21632\n",
            "21696\n",
            "21760\n",
            "21824\n",
            "21888\n",
            "21952\n",
            "22016\n",
            "22080\n",
            "22144\n",
            "22208\n",
            "22272\n",
            "22336\n",
            "22400\n",
            "22464\n",
            "22528\n",
            "22592\n",
            "22656\n",
            "22720\n",
            "22784\n",
            "22848\n",
            "22912\n",
            "22976\n",
            "23040\n",
            "23104\n",
            "23168\n",
            "23232\n",
            "23296\n",
            "23360\n",
            "23424\n",
            "23488\n",
            "23552\n",
            "23616\n",
            "23680\n",
            "23744\n",
            "23808\n",
            "23872\n",
            "23936\n",
            "24000\n",
            "24064\n",
            "24128\n",
            "24192\n",
            "24256\n",
            "24320\n",
            "24384\n",
            "24448\n",
            "24512\n",
            "24576\n",
            "24640\n",
            "24704\n",
            "24768\n",
            "24832\n",
            "24896\n",
            "24960\n",
            "25024\n",
            "25088\n",
            "25152\n",
            "25216\n",
            "25280\n",
            "25344\n",
            "25408\n",
            "25472\n",
            "25536\n",
            "25600\n",
            "25664\n",
            "25728\n",
            "25792\n",
            "25856\n",
            "25920\n",
            "25984\n",
            "26048\n",
            "26112\n",
            "26176\n",
            "26240\n",
            "26304\n",
            "26368\n",
            "26432\n",
            "26496\n",
            "26560\n",
            "26624\n",
            "26688\n",
            "26752\n",
            "26816\n",
            "26880\n",
            "26944\n",
            "27008\n",
            "27072\n",
            "27136\n",
            "27200\n",
            "27264\n",
            "27328\n",
            "27392\n",
            "27456\n",
            "27520\n",
            "27584\n",
            "27648\n",
            "27712\n",
            "27776\n",
            "27840\n",
            "27904\n",
            "27968\n",
            "28032\n",
            "28096\n",
            "28160\n",
            "28224\n",
            "28288\n",
            "28352\n",
            "28416\n",
            "28480\n",
            "28544\n",
            "28608\n",
            "28672\n",
            "28736\n",
            "28800\n",
            "28864\n",
            "28928\n",
            "28992\n",
            "29056\n",
            "29120\n",
            "29184\n",
            "29248\n",
            "29312\n",
            "29376\n",
            "29440\n",
            "29504\n",
            "29568\n",
            "29632\n",
            "29696\n",
            "29760\n",
            "29824\n",
            "29888\n",
            "29952\n",
            "30016\n",
            "30080\n",
            "30144\n",
            "30208\n",
            "30272\n",
            "30336\n",
            "30400\n",
            "30464\n",
            "30528\n",
            "30592\n",
            "30656\n",
            "30720\n",
            "30784\n",
            "30848\n",
            "30912\n",
            "30976\n",
            "31040\n",
            "31104\n",
            "31168\n",
            "31232\n",
            "31296\n",
            "31360\n",
            "31424\n",
            "31488\n",
            "31552\n",
            "31616\n",
            "31680\n",
            "31744\n",
            "31808\n",
            "31872\n",
            "31936\n",
            "32000\n",
            "32064\n",
            "32128\n",
            "32192\n",
            "32256\n",
            "32320\n",
            "32384\n",
            "32448\n",
            "32512\n",
            "32576\n",
            "32640\n",
            "32704\n",
            "32768\n",
            "32832\n",
            "32896\n",
            "32960\n",
            "33024\n",
            "33088\n",
            "33152\n",
            "33216\n",
            "33280\n",
            "33344\n",
            "33408\n",
            "33472\n",
            "33536\n",
            "33600\n",
            "33664\n",
            "33728\n",
            "33792\n",
            "33856\n",
            "33920\n",
            "33984\n",
            "34048\n",
            "34112\n",
            "34176\n",
            "34240\n",
            "34304\n",
            "34368\n",
            "34432\n",
            "34496\n",
            "34560\n",
            "34624\n",
            "34688\n",
            "34752\n",
            "34816\n",
            "34880\n",
            "34944\n",
            "35008\n",
            "35072\n",
            "35136\n",
            "35200\n",
            "35264\n",
            "35328\n",
            "35392\n",
            "35456\n",
            "35520\n",
            "35584\n",
            "35648\n",
            "35712\n",
            "35776\n",
            "35840\n",
            "35904\n",
            "35968\n",
            "36032\n",
            "36096\n",
            "36160\n",
            "36224\n",
            "36288\n",
            "36352\n",
            "36416\n",
            "36480\n",
            "36544\n",
            "36608\n",
            "36672\n",
            "36736\n",
            "36800\n",
            "36864\n",
            "36928\n",
            "36992\n",
            "37056\n",
            "37120\n",
            "37184\n",
            "37248\n",
            "37312\n",
            "37376\n",
            "37440\n",
            "37504\n",
            "37568\n",
            "37632\n",
            "37696\n",
            "37760\n",
            "37824\n",
            "37888\n",
            "37952\n",
            "38016\n",
            "38080\n",
            "38144\n",
            "38208\n",
            "38272\n",
            "38336\n",
            "38400\n",
            "38464\n",
            "38528\n",
            "38592\n",
            "38656\n",
            "38720\n",
            "38784\n",
            "38848\n",
            "38912\n",
            "38976\n",
            "39040\n",
            "39104\n",
            "39168\n",
            "39232\n",
            "39296\n",
            "39360\n",
            "39424\n",
            "39488\n",
            "39552\n",
            "39616\n",
            "39680\n",
            "bb 0.7851980341139058\n",
            "ss 0.7926251691474966\n",
            "fd 0.6512090096058297\n",
            "md 0.7349246231155779\n",
            "fs 0.6941643751909563\n",
            "gmgd 0.28169014084507044\n",
            "gmgs 0.34523809523809523\n",
            "ms 0.6635338345864662\n",
            "gfgs 0.53125\n",
            "gfgd 0.48760330578512395\n",
            "sibs 0.7280248190279214\n",
            "nan 0.7725784516194026\n",
            "0.7448607301914802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPQeyHp0PHn",
        "outputId": "fd2d4cd1-d0c8-4bc4-a987-db819687862e"
      },
      "source": [
        "# ensemble of model_4\n",
        "\n",
        "test_path = \"./kinship_dataset/test-faces/\"\n",
        "submission = pd.read_csv('./kinship_dataset/test-full.csv')\n",
        "predictions_ = {}\n",
        "\n",
        "batch_size = 64\n",
        "for i in range(0, len(submission.p1_new.values), batch_size):\n",
        "    print(i)\n",
        "    X1 = submission.p1_new.values[i:i+batch_size]\n",
        "    X2 = submission.p2_new.values[i:i+batch_size]\n",
        "\n",
        "    input_1 = np.array([read_img(test_path + x) for x in X1])\n",
        "    input_2 = np.array([read_img(test_path + x) for x in X2])\n",
        "\n",
        "    input_1_2 = np.array([np.flip(x, axis=1) for x in input_1])\n",
        "    input_2_2 = np.array([np.flip(x, axis=1) for x in input_2])\n",
        "\n",
        "    input_1_3 = np.array([np.flip(x, axis=-1) for x in input_1])\n",
        "    input_2_3 = np.array([np.flip(x, axis=-1) for x in input_2])\n",
        "\n",
        "    preds = []\n",
        "    for (model, weight) in models[12:16]: \n",
        "        pred1 = model([input_1, input_2], training=False) \n",
        "        pred2 = model([input_1_2, input_2_2], training=False) \n",
        "        pred3 = model([input_1_3, input_2_3], training=False) \n",
        "        pred = (pred1 + pred2 + pred3) * weight * 4 / 3\n",
        "        preds.append(pred)\n",
        "\n",
        "    preds = (np.array(preds).sum(axis=0).squeeze() > 0.47689127922058105).astype(int)\n",
        "    labels = submission.labels[i:i+batch_size]\n",
        "    right_or_wrong = (preds == np.array(labels)).tolist()\n",
        "\n",
        "    ptypes = submission.ptype[i:i+batch_size].tolist()\n",
        "    for j in range(len(ptypes)): \n",
        "        ptype = ptypes[j]\n",
        "        predictions_.setdefault(ptype, {\"count\": 0, \"correct\": 0})\n",
        "        predictions_[ptype][\"correct\"] += right_or_wrong[j]\n",
        "        predictions_[ptype][\"count\"] += 1\n",
        "    \n",
        "correct_sum = 0\n",
        "count_sum = 0    \n",
        "for ptype in predictions_: \n",
        "    print(ptype, predictions_[ptype][\"correct\"] * 1.0/ predictions_[ptype][\"count\"])\n",
        "    correct_sum += predictions_[ptype][\"correct\"] \n",
        "    count_sum += predictions_[ptype][\"count\"]\n",
        "print(correct_sum * 1.0 / count_sum)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "64\n",
            "128\n",
            "192\n",
            "256\n",
            "320\n",
            "384\n",
            "448\n",
            "512\n",
            "576\n",
            "640\n",
            "704\n",
            "768\n",
            "832\n",
            "896\n",
            "960\n",
            "1024\n",
            "1088\n",
            "1152\n",
            "1216\n",
            "1280\n",
            "1344\n",
            "1408\n",
            "1472\n",
            "1536\n",
            "1600\n",
            "1664\n",
            "1728\n",
            "1792\n",
            "1856\n",
            "1920\n",
            "1984\n",
            "2048\n",
            "2112\n",
            "2176\n",
            "2240\n",
            "2304\n",
            "2368\n",
            "2432\n",
            "2496\n",
            "2560\n",
            "2624\n",
            "2688\n",
            "2752\n",
            "2816\n",
            "2880\n",
            "2944\n",
            "3008\n",
            "3072\n",
            "3136\n",
            "3200\n",
            "3264\n",
            "3328\n",
            "3392\n",
            "3456\n",
            "3520\n",
            "3584\n",
            "3648\n",
            "3712\n",
            "3776\n",
            "3840\n",
            "3904\n",
            "3968\n",
            "4032\n",
            "4096\n",
            "4160\n",
            "4224\n",
            "4288\n",
            "4352\n",
            "4416\n",
            "4480\n",
            "4544\n",
            "4608\n",
            "4672\n",
            "4736\n",
            "4800\n",
            "4864\n",
            "4928\n",
            "4992\n",
            "5056\n",
            "5120\n",
            "5184\n",
            "5248\n",
            "5312\n",
            "5376\n",
            "5440\n",
            "5504\n",
            "5568\n",
            "5632\n",
            "5696\n",
            "5760\n",
            "5824\n",
            "5888\n",
            "5952\n",
            "6016\n",
            "6080\n",
            "6144\n",
            "6208\n",
            "6272\n",
            "6336\n",
            "6400\n",
            "6464\n",
            "6528\n",
            "6592\n",
            "6656\n",
            "6720\n",
            "6784\n",
            "6848\n",
            "6912\n",
            "6976\n",
            "7040\n",
            "7104\n",
            "7168\n",
            "7232\n",
            "7296\n",
            "7360\n",
            "7424\n",
            "7488\n",
            "7552\n",
            "7616\n",
            "7680\n",
            "7744\n",
            "7808\n",
            "7872\n",
            "7936\n",
            "8000\n",
            "8064\n",
            "8128\n",
            "8192\n",
            "8256\n",
            "8320\n",
            "8384\n",
            "8448\n",
            "8512\n",
            "8576\n",
            "8640\n",
            "8704\n",
            "8768\n",
            "8832\n",
            "8896\n",
            "8960\n",
            "9024\n",
            "9088\n",
            "9152\n",
            "9216\n",
            "9280\n",
            "9344\n",
            "9408\n",
            "9472\n",
            "9536\n",
            "9600\n",
            "9664\n",
            "9728\n",
            "9792\n",
            "9856\n",
            "9920\n",
            "9984\n",
            "10048\n",
            "10112\n",
            "10176\n",
            "10240\n",
            "10304\n",
            "10368\n",
            "10432\n",
            "10496\n",
            "10560\n",
            "10624\n",
            "10688\n",
            "10752\n",
            "10816\n",
            "10880\n",
            "10944\n",
            "11008\n",
            "11072\n",
            "11136\n",
            "11200\n",
            "11264\n",
            "11328\n",
            "11392\n",
            "11456\n",
            "11520\n",
            "11584\n",
            "11648\n",
            "11712\n",
            "11776\n",
            "11840\n",
            "11904\n",
            "11968\n",
            "12032\n",
            "12096\n",
            "12160\n",
            "12224\n",
            "12288\n",
            "12352\n",
            "12416\n",
            "12480\n",
            "12544\n",
            "12608\n",
            "12672\n",
            "12736\n",
            "12800\n",
            "12864\n",
            "12928\n",
            "12992\n",
            "13056\n",
            "13120\n",
            "13184\n",
            "13248\n",
            "13312\n",
            "13376\n",
            "13440\n",
            "13504\n",
            "13568\n",
            "13632\n",
            "13696\n",
            "13760\n",
            "13824\n",
            "13888\n",
            "13952\n",
            "14016\n",
            "14080\n",
            "14144\n",
            "14208\n",
            "14272\n",
            "14336\n",
            "14400\n",
            "14464\n",
            "14528\n",
            "14592\n",
            "14656\n",
            "14720\n",
            "14784\n",
            "14848\n",
            "14912\n",
            "14976\n",
            "15040\n",
            "15104\n",
            "15168\n",
            "15232\n",
            "15296\n",
            "15360\n",
            "15424\n",
            "15488\n",
            "15552\n",
            "15616\n",
            "15680\n",
            "15744\n",
            "15808\n",
            "15872\n",
            "15936\n",
            "16000\n",
            "16064\n",
            "16128\n",
            "16192\n",
            "16256\n",
            "16320\n",
            "16384\n",
            "16448\n",
            "16512\n",
            "16576\n",
            "16640\n",
            "16704\n",
            "16768\n",
            "16832\n",
            "16896\n",
            "16960\n",
            "17024\n",
            "17088\n",
            "17152\n",
            "17216\n",
            "17280\n",
            "17344\n",
            "17408\n",
            "17472\n",
            "17536\n",
            "17600\n",
            "17664\n",
            "17728\n",
            "17792\n",
            "17856\n",
            "17920\n",
            "17984\n",
            "18048\n",
            "18112\n",
            "18176\n",
            "18240\n",
            "18304\n",
            "18368\n",
            "18432\n",
            "18496\n",
            "18560\n",
            "18624\n",
            "18688\n",
            "18752\n",
            "18816\n",
            "18880\n",
            "18944\n",
            "19008\n",
            "19072\n",
            "19136\n",
            "19200\n",
            "19264\n",
            "19328\n",
            "19392\n",
            "19456\n",
            "19520\n",
            "19584\n",
            "19648\n",
            "19712\n",
            "19776\n",
            "19840\n",
            "19904\n",
            "19968\n",
            "20032\n",
            "20096\n",
            "20160\n",
            "20224\n",
            "20288\n",
            "20352\n",
            "20416\n",
            "20480\n",
            "20544\n",
            "20608\n",
            "20672\n",
            "20736\n",
            "20800\n",
            "20864\n",
            "20928\n",
            "20992\n",
            "21056\n",
            "21120\n",
            "21184\n",
            "21248\n",
            "21312\n",
            "21376\n",
            "21440\n",
            "21504\n",
            "21568\n",
            "21632\n",
            "21696\n",
            "21760\n",
            "21824\n",
            "21888\n",
            "21952\n",
            "22016\n",
            "22080\n",
            "22144\n",
            "22208\n",
            "22272\n",
            "22336\n",
            "22400\n",
            "22464\n",
            "22528\n",
            "22592\n",
            "22656\n",
            "22720\n",
            "22784\n",
            "22848\n",
            "22912\n",
            "22976\n",
            "23040\n",
            "23104\n",
            "23168\n",
            "23232\n",
            "23296\n",
            "23360\n",
            "23424\n",
            "23488\n",
            "23552\n",
            "23616\n",
            "23680\n",
            "23744\n",
            "23808\n",
            "23872\n",
            "23936\n",
            "24000\n",
            "24064\n",
            "24128\n",
            "24192\n",
            "24256\n",
            "24320\n",
            "24384\n",
            "24448\n",
            "24512\n",
            "24576\n",
            "24640\n",
            "24704\n",
            "24768\n",
            "24832\n",
            "24896\n",
            "24960\n",
            "25024\n",
            "25088\n",
            "25152\n",
            "25216\n",
            "25280\n",
            "25344\n",
            "25408\n",
            "25472\n",
            "25536\n",
            "25600\n",
            "25664\n",
            "25728\n",
            "25792\n",
            "25856\n",
            "25920\n",
            "25984\n",
            "26048\n",
            "26112\n",
            "26176\n",
            "26240\n",
            "26304\n",
            "26368\n",
            "26432\n",
            "26496\n",
            "26560\n",
            "26624\n",
            "26688\n",
            "26752\n",
            "26816\n",
            "26880\n",
            "26944\n",
            "27008\n",
            "27072\n",
            "27136\n",
            "27200\n",
            "27264\n",
            "27328\n",
            "27392\n",
            "27456\n",
            "27520\n",
            "27584\n",
            "27648\n",
            "27712\n",
            "27776\n",
            "27840\n",
            "27904\n",
            "27968\n",
            "28032\n",
            "28096\n",
            "28160\n",
            "28224\n",
            "28288\n",
            "28352\n",
            "28416\n",
            "28480\n",
            "28544\n",
            "28608\n",
            "28672\n",
            "28736\n",
            "28800\n",
            "28864\n",
            "28928\n",
            "28992\n",
            "29056\n",
            "29120\n",
            "29184\n",
            "29248\n",
            "29312\n",
            "29376\n",
            "29440\n",
            "29504\n",
            "29568\n",
            "29632\n",
            "29696\n",
            "29760\n",
            "29824\n",
            "29888\n",
            "29952\n",
            "30016\n",
            "30080\n",
            "30144\n",
            "30208\n",
            "30272\n",
            "30336\n",
            "30400\n",
            "30464\n",
            "30528\n",
            "30592\n",
            "30656\n",
            "30720\n",
            "30784\n",
            "30848\n",
            "30912\n",
            "30976\n",
            "31040\n",
            "31104\n",
            "31168\n",
            "31232\n",
            "31296\n",
            "31360\n",
            "31424\n",
            "31488\n",
            "31552\n",
            "31616\n",
            "31680\n",
            "31744\n",
            "31808\n",
            "31872\n",
            "31936\n",
            "32000\n",
            "32064\n",
            "32128\n",
            "32192\n",
            "32256\n",
            "32320\n",
            "32384\n",
            "32448\n",
            "32512\n",
            "32576\n",
            "32640\n",
            "32704\n",
            "32768\n",
            "32832\n",
            "32896\n",
            "32960\n",
            "33024\n",
            "33088\n",
            "33152\n",
            "33216\n",
            "33280\n",
            "33344\n",
            "33408\n",
            "33472\n",
            "33536\n",
            "33600\n",
            "33664\n",
            "33728\n",
            "33792\n",
            "33856\n",
            "33920\n",
            "33984\n",
            "34048\n",
            "34112\n",
            "34176\n",
            "34240\n",
            "34304\n",
            "34368\n",
            "34432\n",
            "34496\n",
            "34560\n",
            "34624\n",
            "34688\n",
            "34752\n",
            "34816\n",
            "34880\n",
            "34944\n",
            "35008\n",
            "35072\n",
            "35136\n",
            "35200\n",
            "35264\n",
            "35328\n",
            "35392\n",
            "35456\n",
            "35520\n",
            "35584\n",
            "35648\n",
            "35712\n",
            "35776\n",
            "35840\n",
            "35904\n",
            "35968\n",
            "36032\n",
            "36096\n",
            "36160\n",
            "36224\n",
            "36288\n",
            "36352\n",
            "36416\n",
            "36480\n",
            "36544\n",
            "36608\n",
            "36672\n",
            "36736\n",
            "36800\n",
            "36864\n",
            "36928\n",
            "36992\n",
            "37056\n",
            "37120\n",
            "37184\n",
            "37248\n",
            "37312\n",
            "37376\n",
            "37440\n",
            "37504\n",
            "37568\n",
            "37632\n",
            "37696\n",
            "37760\n",
            "37824\n",
            "37888\n",
            "37952\n",
            "38016\n",
            "38080\n",
            "38144\n",
            "38208\n",
            "38272\n",
            "38336\n",
            "38400\n",
            "38464\n",
            "38528\n",
            "38592\n",
            "38656\n",
            "38720\n",
            "38784\n",
            "38848\n",
            "38912\n",
            "38976\n",
            "39040\n",
            "39104\n",
            "39168\n",
            "39232\n",
            "39296\n",
            "39360\n",
            "39424\n",
            "39488\n",
            "39552\n",
            "39616\n",
            "39680\n",
            "bb 0.8427291124602486\n",
            "ss 0.834573748308525\n",
            "fd 0.696588274263001\n",
            "md 0.7679020100502513\n",
            "fs 0.773602199816682\n",
            "gmgd 0.29577464788732394\n",
            "gmgs 0.30952380952380953\n",
            "ms 0.7\n",
            "gfgs 0.5520833333333334\n",
            "gfgd 0.4049586776859504\n",
            "sibs 0.7807652533609101\n",
            "nan 0.746234825970886\n",
            "0.7559318622147296\n"
          ]
        }
      ]
    }
  ]
}